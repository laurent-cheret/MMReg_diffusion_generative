{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM-Reg: Photonics Dataset Experiment\n",
    "\n",
    "Baseline VAE vs MM-Reg VAE for latent diffusion on binary photonics images.\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "Image (1x64x64) -> SimpleConvVAE -> 64-d latent -> MLP Diffusion -> Generate\n",
    "```\n",
    "\n",
    "## Pipeline\n",
    "1. **Load Data**: Load `imagenorm.npy`, resize to 64x64, threshold to binary\n",
    "2. **PCA Reference**: Compute PCA embeddings for MM-Reg\n",
    "3. **Train VAEs**: Baseline (no MM-Reg) vs MM-Reg\n",
    "4. **Evaluate VAEs**: Reconstruction quality, distance correlation\n",
    "5. **Train Diffusion**: MLP denoiser on 64-d latents from each VAE\n",
    "6. **Generate & Compare**: Sample quality from both pipelines\n",
    "\n",
    "**Hypothesis**: MM-Reg creates smoother latent space -> diffusion learns faster/better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.models.losses import MMRegLoss, pairwise_distances, get_upper_triangular\n",
    "from src.models.diffusion import MLPDenoiser, GaussianDiffusion\n",
    "from src.diffusion_trainer import DiffusionTrainer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Data\n",
    "    'image_size': 64,\n",
    "    'train_split': 0.8,\n",
    "    'batch_size': 32,\n",
    "\n",
    "    # PCA\n",
    "    'pca_components': 64,\n",
    "\n",
    "    # VAE\n",
    "    'latent_dim': 64,\n",
    "    'vae_epochs': 200,\n",
    "    'vae_lr': 1e-3,\n",
    "    'lambda_mm': 1.0,\n",
    "    'beta': 0.001,\n",
    "\n",
    "    # Diffusion\n",
    "    'diffusion_epochs': 300,\n",
    "    'diffusion_lr': 1e-4,\n",
    "    'diffusion_timesteps': 1000,\n",
    "}\n",
    "\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Preprocess Photonics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "raw = np.load('../imagenorm.npy')\n",
    "print(f\"Raw shape: {raw.shape}, dtype: {raw.dtype}\")\n",
    "print(f\"Range: [{raw.min():.4f}, {raw.max():.4f}]\")\n",
    "\n",
    "# Resize to 64x64 and threshold to binary\n",
    "from PIL import Image\n",
    "\n",
    "size = CONFIG['image_size']\n",
    "processed = []\n",
    "for i in range(raw.shape[0]):\n",
    "    img = Image.fromarray(raw[i]).resize((size, size), Image.BILINEAR)\n",
    "    arr = np.array(img, dtype=np.float32)\n",
    "    arr = (arr > 0.5).astype(np.float32)  # threshold to binary\n",
    "    processed.append(arr)\n",
    "\n",
    "data = np.stack(processed)  # (N, 64, 64)\n",
    "print(f\"Processed: {data.shape}, unique values: {np.unique(data)}\")\n",
    "\n",
    "# Convert to tensor: (N, 1, 64, 64)\n",
    "data_tensor = torch.from_numpy(data).unsqueeze(1).float()\n",
    "print(f\"Tensor shape: {data_tensor.shape}\")\n",
    "\n",
    "# Train/val split\n",
    "n = len(data_tensor)\n",
    "n_train = int(n * CONFIG['train_split'])\n",
    "perm = torch.randperm(n)\n",
    "train_idx, val_idx = perm[:n_train], perm[n_train:]\n",
    "\n",
    "train_images = data_tensor[train_idx]\n",
    "val_images = data_tensor[val_idx]\n",
    "print(f\"Train: {train_images.shape[0]}, Val: {val_images.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(train_images[i, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'#{i}')\n",
    "plt.suptitle('Processed Training Samples (64x64 binary)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute PCA Reference Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = CONFIG['pca_components']\n",
    "\n",
    "# Flatten images for PCA\n",
    "train_flat = train_images.view(train_images.shape[0], -1).numpy()\n",
    "val_flat = val_images.view(val_images.shape[0], -1).numpy()\n",
    "\n",
    "pca = PCA(n_components=n_comp)\n",
    "train_pca = torch.from_numpy(pca.fit_transform(train_flat)).float()\n",
    "val_pca = torch.from_numpy(pca.transform(val_flat)).float()\n",
    "\n",
    "print(f\"Train PCA: {train_pca.shape}, Val PCA: {val_pca.shape}\")\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define SimpleConvVAE\n",
    "\n",
    "Lightweight convolutional VAE for single-channel 64x64 binary images.\n",
    "Encodes directly to a flat latent vector (no SD VAE needed for these simple images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvVAE(nn.Module):\n",
    "    \"\"\"Conv VAE: 1x64x64 -> latent_dim -> 1x64x64.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder: 1x64x64 -> 256x4x4\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(True),   # -> 32x32\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(True),  # -> 16x16\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(True),# -> 8x8\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(True),# -> 4x4\n",
    "        )\n",
    "        self.flat_dim = 256 * 4 * 4\n",
    "        self.fc_mu = nn.Linear(self.flat_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flat_dim, latent_dim)\n",
    "\n",
    "        # Decoder: latent_dim -> 1x64x64\n",
    "        self.fc_decode = nn.Linear(latent_dim, self.flat_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(True),  # -> 8x8\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(True),    # -> 16x16\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(True),     # -> 32x32\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1), nn.Sigmoid(),                            # -> 64x64\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x).view(-1, self.flat_dim)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            return mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
    "        return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_decode(z).view(-1, 256, 4, 4)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar, z\n",
    "\n",
    "\n",
    "# Quick test\n",
    "_vae = SimpleConvVAE(CONFIG['latent_dim'])\n",
    "_out = _vae(torch.randn(2, 1, 64, 64))\n",
    "print(f\"VAE params: {sum(p.numel() for p in _vae.parameters()):,}\")\n",
    "print(f\"Input: (2, 1, 64, 64) -> Recon: {_out[0].shape}, mu: {_out[1].shape}\")\n",
    "del _vae, _out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. VAE Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_loss_fn = MMRegLoss(variant='correlation')\n",
    "\n",
    "\n",
    "def train_vae(model, train_images, train_pca, val_images, val_pca,\n",
    "              config, use_mmreg=False, device='cuda', save_dir=None):\n",
    "    \"\"\"Train a SimpleConvVAE with optional MM-Reg.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['vae_lr'])\n",
    "    bs = config['batch_size']\n",
    "    beta = config['beta']\n",
    "    lam = config['lambda_mm'] if use_mmreg else 0.0\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    history = {'train': [], 'val': []}\n",
    "    best_val = float('inf')\n",
    "\n",
    "    for epoch in range(config['vae_epochs']):\n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        perm = torch.randperm(len(train_images))\n",
    "        epoch_loss = {'loss': 0, 'recon': 0, 'kl': 0, 'mm': 0}\n",
    "        n_batches = 0\n",
    "\n",
    "        for i in range(0, len(train_images), bs):\n",
    "            idx = perm[i:i+bs]\n",
    "            imgs = train_images[idx].to(device)\n",
    "            pca_emb = train_pca[idx].to(device)\n",
    "\n",
    "            x_recon, mu, logvar, z = model(imgs)\n",
    "\n",
    "            recon = F.mse_loss(x_recon, imgs)\n",
    "            kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()\n",
    "            mm = mm_loss_fn(z, pca_emb) if lam > 0 else torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = recon + beta * kl + lam * mm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss['loss'] += loss.item()\n",
    "            epoch_loss['recon'] += recon.item()\n",
    "            epoch_loss['kl'] += kl.item()\n",
    "            epoch_loss['mm'] += mm.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        train_metrics = {k: v / n_batches for k, v in epoch_loss.items()}\n",
    "        history['train'].append(train_metrics)\n",
    "\n",
    "        # --- Validate ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = {'loss': 0, 'recon': 0, 'kl': 0, 'mm': 0}\n",
    "            n_val = 0\n",
    "            for i in range(0, len(val_images), bs):\n",
    "                imgs = val_images[i:i+bs].to(device)\n",
    "                pca_emb = val_pca[i:i+bs].to(device)\n",
    "\n",
    "                x_recon, mu, logvar, z = model(imgs)\n",
    "\n",
    "                recon = F.mse_loss(x_recon, imgs)\n",
    "                kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()\n",
    "                mm = mm_loss_fn(z, pca_emb) if lam > 0 else torch.tensor(0.0, device=device)\n",
    "\n",
    "                val_loss['loss'] += (recon + beta * kl + lam * mm).item()\n",
    "                val_loss['recon'] += recon.item()\n",
    "                val_loss['kl'] += kl.item()\n",
    "                val_loss['mm'] += mm.item()\n",
    "                n_val += 1\n",
    "\n",
    "            val_metrics = {k: v / n_val for k, v in val_loss.items()}\n",
    "            history['val'].append(val_metrics)\n",
    "\n",
    "        if val_metrics['loss'] < best_val:\n",
    "            best_val = val_metrics['loss']\n",
    "            if save_dir:\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, 'best.pt'))\n",
    "\n",
    "        if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "            tag = 'MMReg' if use_mmreg else 'Baseline'\n",
    "            print(f\"[{tag}] Epoch {epoch+1}/{config['vae_epochs']} - \"\n",
    "                  f\"Train: loss={train_metrics['loss']:.4f} recon={train_metrics['recon']:.4f} \"\n",
    "                  f\"kl={train_metrics['kl']:.2f} mm={train_metrics['mm']:.4f} | \"\n",
    "                  f\"Val: loss={val_metrics['loss']:.4f} recon={val_metrics['recon']:.4f}\")\n",
    "\n",
    "    if save_dir:\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, 'final.pt'))\n",
    "        with open(os.path.join(save_dir, 'history.json'), 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Baseline VAE (no MM-Reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING BASELINE VAE (no MM-Reg)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vae_baseline = SimpleConvVAE(CONFIG['latent_dim'])\n",
    "hist_baseline = train_vae(\n",
    "    vae_baseline, train_images, train_pca, val_images, val_pca,\n",
    "    CONFIG, use_mmreg=False, device=device,\n",
    "    save_dir='../checkpoints/photonics_baseline'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train MM-Reg VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING MM-REG VAE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vae_mmreg = SimpleConvVAE(CONFIG['latent_dim'])\n",
    "hist_mmreg = train_vae(\n",
    "    vae_mmreg, train_images, train_pca, val_images, val_pca,\n",
    "    CONFIG, use_mmreg=True, device=device,\n",
    "    save_dir='../checkpoints/photonics_mmreg'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vae(model, images, pca_emb, name, device):\n",
    "    \"\"\"Evaluate reconstruction and distance correlation.\"\"\"\n",
    "    model.eval()\n",
    "    all_z = []\n",
    "    total_mse = 0\n",
    "    total_acc = 0\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), 64):\n",
    "            batch = images[i:i+64].to(device)\n",
    "            x_recon, mu, logvar, z = model(batch)\n",
    "            total_mse += F.mse_loss(x_recon, batch, reduction='sum').item()\n",
    "            # Pixel accuracy (threshold at 0.5)\n",
    "            pred_binary = (x_recon > 0.5).float()\n",
    "            total_acc += (pred_binary == batch).float().sum().item()\n",
    "            n += batch.shape[0]\n",
    "            all_z.append(mu.cpu())\n",
    "\n",
    "    all_z = torch.cat(all_z, dim=0)\n",
    "    pixel_count = images.shape[0] * images.shape[1] * images.shape[2] * images.shape[3]\n",
    "\n",
    "    # Distance correlation\n",
    "    n_corr = min(200, len(all_z))\n",
    "    D_z = pairwise_distances(all_z[:n_corr])\n",
    "    D_pca = pairwise_distances(pca_emb[:n_corr])\n",
    "    d_z = get_upper_triangular(D_z).numpy()\n",
    "    d_pca = get_upper_triangular(D_pca).numpy()\n",
    "    pearson, _ = pearsonr(d_z, d_pca)\n",
    "    spearman, _ = spearmanr(d_z, d_pca)\n",
    "\n",
    "    results = {\n",
    "        'mse': total_mse / pixel_count,\n",
    "        'pixel_acc': total_acc / pixel_count,\n",
    "        'pearson': pearson,\n",
    "        'spearman': spearman,\n",
    "        'latents': all_z,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"  Recon MSE:      {results['mse']:.6f}\")\n",
    "    print(f\"  Pixel Accuracy: {results['pixel_acc']:.4f}\")\n",
    "    print(f\"  Pearson corr:   {results['pearson']:.4f}\")\n",
    "    print(f\"  Spearman corr:  {results['spearman']:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "res_baseline = evaluate_vae(vae_baseline, val_images, val_pca, \"Baseline\", device)\n",
    "res_mmreg = evaluate_vae(vae_mmreg, val_images, val_pca, \"MM-Reg\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "def plot_reconstructions(model, images, title, device):\n",
    "    model.eval()\n",
    "    imgs = images[:8].to(device)\n",
    "    with torch.no_grad():\n",
    "        recon, _, _, _ = model(imgs)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    for i in range(8):\n",
    "        axes[0, i].imshow(imgs[i, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(recon[i, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "        axes[1, i].axis('off')\n",
    "        axes[2, i].imshow((recon[i, 0].cpu() > 0.5).float(), cmap='gray', vmin=0, vmax=1)\n",
    "        axes[2, i].axis('off')\n",
    "    axes[0, 0].set_ylabel('Original', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('Recon (raw)', fontsize=11)\n",
    "    axes[2, 0].set_ylabel('Recon (binary)', fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_reconstructions(vae_baseline, val_images, \"Baseline VAE Reconstructions\", device)\n",
    "plot_reconstructions(vae_mmreg, val_images, \"MM-Reg VAE Reconstructions\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance correlation scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for ax, z, name in [(axes[0], res_baseline['latents'], 'Baseline'),\n",
    "                     (axes[1], res_mmreg['latents'], 'MM-Reg')]:\n",
    "    n_pts = min(200, len(z))\n",
    "    D_z = get_upper_triangular(pairwise_distances(z[:n_pts])).numpy()\n",
    "    D_pca = get_upper_triangular(pairwise_distances(val_pca[:n_pts])).numpy()\n",
    "    r, _ = pearsonr(D_z, D_pca)\n",
    "\n",
    "    ax.scatter(D_pca, D_z, alpha=0.05, s=2)\n",
    "    ax.set_xlabel('PCA distances')\n",
    "    ax.set_ylabel('Latent distances')\n",
    "    ax.set_title(f'{name} (r={r:.4f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Pairwise Distance Correlation (Val Set)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Encode Datasets to Latent Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_all(model, images, device, batch_size=64):\n",
    "    \"\"\"Encode images to latent mu vectors.\"\"\"\n",
    "    model.eval()\n",
    "    zs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i+batch_size].to(device)\n",
    "        mu, _ = model.encode(batch)\n",
    "        zs.append(mu.cpu())\n",
    "    return torch.cat(zs, dim=0)\n",
    "\n",
    "\n",
    "train_z_baseline = encode_all(vae_baseline, train_images, device)\n",
    "val_z_baseline = encode_all(vae_baseline, val_images, device)\n",
    "print(f\"Baseline latents - Train: {train_z_baseline.shape}, Val: {val_z_baseline.shape}\")\n",
    "\n",
    "train_z_mmreg = encode_all(vae_mmreg, train_images, device)\n",
    "val_z_mmreg = encode_all(vae_mmreg, val_images, device)\n",
    "print(f\"MM-Reg latents   - Train: {train_z_mmreg.shape}, Val: {val_z_mmreg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Diffusion on Baseline Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING DIFFUSION ON BASELINE LATENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diffusion_baseline = GaussianDiffusion(\n",
    "    num_timesteps=CONFIG['diffusion_timesteps'], device=device\n",
    ")\n",
    "\n",
    "mlp_baseline = MLPDenoiser(\n",
    "    input_dim=CONFIG['latent_dim'],\n",
    "    hidden_dim=512,\n",
    "    num_layers=4,\n",
    "    time_emb_dim=128\n",
    ").to(device)\n",
    "\n",
    "print(f\"MLP params: {sum(p.numel() for p in mlp_baseline.parameters()):,}\")\n",
    "\n",
    "opt_diff_base = torch.optim.AdamW(mlp_baseline.parameters(), lr=CONFIG['diffusion_lr'])\n",
    "\n",
    "trainer_diff_baseline = DiffusionTrainer(\n",
    "    model=mlp_baseline,\n",
    "    diffusion=diffusion_baseline,\n",
    "    optimizer=opt_diff_base,\n",
    "    train_latents=train_z_baseline,\n",
    "    val_latents=val_z_baseline,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    device=device,\n",
    "    use_amp=False,\n",
    "    save_dir='../checkpoints/photonics_diffusion_baseline'\n",
    ")\n",
    "\n",
    "trainer_diff_baseline.train(num_epochs=CONFIG['diffusion_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train Diffusion on MM-Reg Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING DIFFUSION ON MM-REG LATENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diffusion_mmreg = GaussianDiffusion(\n",
    "    num_timesteps=CONFIG['diffusion_timesteps'], device=device\n",
    ")\n",
    "\n",
    "mlp_mmreg = MLPDenoiser(\n",
    "    input_dim=CONFIG['latent_dim'],\n",
    "    hidden_dim=512,\n",
    "    num_layers=4,\n",
    "    time_emb_dim=128\n",
    ").to(device)\n",
    "\n",
    "opt_diff_mm = torch.optim.AdamW(mlp_mmreg.parameters(), lr=CONFIG['diffusion_lr'])\n",
    "\n",
    "trainer_diff_mmreg = DiffusionTrainer(\n",
    "    model=mlp_mmreg,\n",
    "    diffusion=diffusion_mmreg,\n",
    "    optimizer=opt_diff_mm,\n",
    "    train_latents=train_z_mmreg,\n",
    "    val_latents=val_z_mmreg,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    device=device,\n",
    "    use_amp=False,\n",
    "    save_dir='../checkpoints/photonics_diffusion_mmreg'\n",
    ")\n",
    "\n",
    "trainer_diff_mmreg.train(num_epochs=CONFIG['diffusion_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Samples & Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate latent vectors from diffusion\n",
    "print(\"Generating from Baseline Diffusion...\")\n",
    "gen_z_baseline = trainer_diff_baseline.generate_samples(num_samples=16)\n",
    "\n",
    "print(\"Generating from MM-Reg Diffusion...\")\n",
    "gen_z_mmreg = trainer_diff_mmreg.generate_samples(num_samples=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def decode_and_plot(vae, z_vectors, title, device):\n",
    "    \"\"\"Decode latent vectors to images and plot.\"\"\"\n",
    "    vae.eval()\n",
    "    z = z_vectors.to(device)\n",
    "    images = vae.decode(z).cpu()\n",
    "\n",
    "    n = min(16, images.shape[0])\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "\n",
    "    for i in range(n):\n",
    "        row, col = i // 8, i % 8\n",
    "        # Raw output\n",
    "        axes[row, col].imshow(images[i, 0].clamp(0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Also show thresholded (binary) versions\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    fig.suptitle(f\"{title} (thresholded)\", fontsize=14)\n",
    "\n",
    "    for i in range(n):\n",
    "        row, col = i // 8, i % 8\n",
    "        axes[row, col].imshow((images[i, 0] > 0.5).float(), cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "decode_and_plot(vae_baseline, gen_z_baseline,\n",
    "                \"Generated Samples (Baseline VAE + Diffusion)\", device)\n",
    "decode_and_plot(vae_mmreg, gen_z_mmreg,\n",
    "                \"Generated Samples (MM-Reg VAE + Diffusion)\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: Real vs Baseline Generated vs MM-Reg Generated\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "\n",
    "# Real samples\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(val_images[i, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Baseline generated (thresholded)\n",
    "vae_baseline.eval()\n",
    "with torch.no_grad():\n",
    "    baseline_imgs = vae_baseline.decode(gen_z_baseline[:8].to(device)).cpu()\n",
    "for i in range(8):\n",
    "    axes[1, i].imshow((baseline_imgs[i, 0] > 0.5).float(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# MM-Reg generated (thresholded)\n",
    "vae_mmreg.eval()\n",
    "with torch.no_grad():\n",
    "    mmreg_imgs = vae_mmreg.decode(gen_z_mmreg[:8].to(device)).cpu()\n",
    "for i in range(8):\n",
    "    axes[2, i].imshow((mmreg_imgs[i, 0] > 0.5).float(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Real', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Baseline', fontsize=12)\n",
    "axes[2, 0].set_ylabel('MM-Reg', fontsize=12)\n",
    "plt.suptitle('Real vs Generated Photonics Patterns', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Latent Space Interpolation\n",
    "\n",
    "Interpolate between two latent vectors and decode. A well-structured latent space\n",
    "(MM-Reg) should produce smooth, realistic transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_interpolation(vae, z_start, z_end, title, device, steps=8):\n",
    "    vae.eval()\n",
    "    alphas = torch.linspace(0, 1, steps)\n",
    "    z_interp = torch.stack([z_start * (1 - a) + z_end * a for a in alphas]).to(device)\n",
    "    imgs = vae.decode(z_interp).cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(1, steps, figsize=(2 * steps, 2))\n",
    "    fig.suptitle(title, fontsize=12)\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow((imgs[i, 0] > 0.5).float(), cmap='gray', vmin=0, vmax=1)\n",
    "        ax.set_title(f'a={alphas[i]:.1f}')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Pick two val samples and interpolate in each latent space\n",
    "z0_b, z1_b = val_z_baseline[0], val_z_baseline[5]\n",
    "z0_m, z1_m = val_z_mmreg[0], val_z_mmreg[5]\n",
    "\n",
    "plot_interpolation(vae_baseline, z0_b, z1_b, \"Baseline Interpolation\", device)\n",
    "plot_interpolation(vae_mmreg, z0_m, z1_m, \"MM-Reg Interpolation\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary & Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# VAE total loss\n",
    "axes[0].plot([h['loss'] for h in hist_baseline['train']], 'b-', alpha=0.7, label='Baseline Train')\n",
    "axes[0].plot([h['loss'] for h in hist_baseline['val']], 'b--', alpha=0.7, label='Baseline Val')\n",
    "axes[0].plot([h['loss'] for h in hist_mmreg['train']], 'r-', alpha=0.7, label='MM-Reg Train')\n",
    "axes[0].plot([h['loss'] for h in hist_mmreg['val']], 'r--', alpha=0.7, label='MM-Reg Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('VAE Total Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# VAE recon loss\n",
    "axes[1].plot([h['recon'] for h in hist_baseline['train']], 'b-', alpha=0.7, label='Baseline Train')\n",
    "axes[1].plot([h['recon'] for h in hist_baseline['val']], 'b--', alpha=0.7, label='Baseline Val')\n",
    "axes[1].plot([h['recon'] for h in hist_mmreg['train']], 'r-', alpha=0.7, label='MM-Reg Train')\n",
    "axes[1].plot([h['recon'] for h in hist_mmreg['val']], 'r--', alpha=0.7, label='MM-Reg Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Recon MSE')\n",
    "axes[1].set_title('VAE Reconstruction Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Diffusion loss\n",
    "diff_hist_base = trainer_diff_baseline.train_history\n",
    "diff_hist_mm = trainer_diff_mmreg.train_history\n",
    "diff_val_base = trainer_diff_baseline.val_history\n",
    "diff_val_mm = trainer_diff_mmreg.val_history\n",
    "\n",
    "axes[2].plot([h['loss'] for h in diff_hist_base], 'b-', alpha=0.7, label='Baseline Train')\n",
    "axes[2].plot([h['loss'] for h in diff_val_base], 'b--', alpha=0.7, label='Baseline Val')\n",
    "axes[2].plot([h['loss'] for h in diff_hist_mm], 'r-', alpha=0.7, label='MM-Reg Train')\n",
    "axes[2].plot([h['loss'] for h in diff_val_mm], 'r--', alpha=0.7, label='MM-Reg Val')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_title('Diffusion Training Loss')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../checkpoints/photonics_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_python(v):\n",
    "    return v.item() if hasattr(v, 'item') else float(v)\n",
    "\n",
    "summary = {\n",
    "    'config': CONFIG,\n",
    "    'dataset': {'total': len(data_tensor), 'train': len(train_images), 'val': len(val_images)},\n",
    "    'vae_results': {\n",
    "        'baseline': {\n",
    "            'recon_mse': to_python(res_baseline['mse']),\n",
    "            'pixel_acc': to_python(res_baseline['pixel_acc']),\n",
    "            'pearson': to_python(res_baseline['pearson']),\n",
    "            'spearman': to_python(res_baseline['spearman']),\n",
    "        },\n",
    "        'mmreg': {\n",
    "            'recon_mse': to_python(res_mmreg['mse']),\n",
    "            'pixel_acc': to_python(res_mmreg['pixel_acc']),\n",
    "            'pearson': to_python(res_mmreg['pearson']),\n",
    "            'spearman': to_python(res_mmreg['spearman']),\n",
    "        },\n",
    "    },\n",
    "    'diffusion_final_loss': {\n",
    "        'baseline_train': diff_hist_base[-1]['loss'],\n",
    "        'baseline_val': diff_val_base[-1]['loss'],\n",
    "        'mmreg_train': diff_hist_mm[-1]['loss'],\n",
    "        'mmreg_val': diff_val_mm[-1]['loss'],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHOTONICS EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: {summary['dataset']}\")\n",
    "\n",
    "print(\"\\nVAE Comparison (val set):\")\n",
    "print(f\"  Baseline - MSE: {res_baseline['mse']:.6f}, Acc: {res_baseline['pixel_acc']:.4f}, \"\n",
    "      f\"Pearson: {res_baseline['pearson']:.4f}\")\n",
    "print(f\"  MM-Reg   - MSE: {res_mmreg['mse']:.6f}, Acc: {res_mmreg['pixel_acc']:.4f}, \"\n",
    "      f\"Pearson: {res_mmreg['pearson']:.4f}\")\n",
    "\n",
    "print(\"\\nDiffusion Final Val Loss:\")\n",
    "print(f\"  Baseline: {diff_val_base[-1]['loss']:.6f}\")\n",
    "print(f\"  MM-Reg:   {diff_val_mm[-1]['loss']:.6f}\")\n",
    "\n",
    "base_val = diff_val_base[-1]['loss']\n",
    "mm_val = diff_val_mm[-1]['loss']\n",
    "improvement = (base_val - mm_val) / base_val * 100\n",
    "print(f\"\\nDiffusion improvement: {improvement:+.1f}%\")\n",
    "\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "with open('../checkpoints/photonics_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved to ../checkpoints/photonics_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}