{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM-Reg: Complete Experiment\n",
    "\n",
    "End-to-end experiment comparing **Baseline VAE** vs **MM-Reg VAE** for latent diffusion.\n",
    "\n",
    "## Pipeline:\n",
    "1. **Setup**: Install dependencies, load data\n",
    "2. **Pre-compute PCA**: Reference embeddings for MM-Reg\n",
    "3. **Train VAEs**: Baseline (no MM-Reg) and MM-Reg versions\n",
    "4. **Evaluate VAEs**: Reconstruction quality, distance correlation\n",
    "5. **Train Diffusion**: On latents from both VAEs\n",
    "6. **Evaluate Diffusion**: Generate samples, compare quality\n",
    "\n",
    "**Hypothesis**: MM-Reg VAE creates smoother latent space â†’ diffusion learns faster/better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf MMReg_diffusion_generative 2>/dev/null\n",
    "!git clone https://github.com/laurent-cheret/MMReg_diffusion_generative.git\n",
    "%cd MMReg_diffusion_generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision diffusers transformers accelerate\n",
    "!pip install -q pyyaml tqdm scipy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_root': './data',\n",
    "    'image_size': 256,\n",
    "    'batch_size': 32,\n",
    "    \n",
    "    # PCA\n",
    "    'pca_components': 256,\n",
    "    \n",
    "    # VAE Training\n",
    "    'vae_epochs': 5,\n",
    "    'vae_lr': 1e-5,\n",
    "    'lambda_mm': 1.0,\n",
    "    'beta': 1e-6,\n",
    "    \n",
    "    # Diffusion Training\n",
    "    'diffusion_epochs': 20,\n",
    "    'diffusion_lr': 1e-4,\n",
    "    'diffusion_timesteps': 1000,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data & Pre-compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataset import (\n",
    "    get_imagenette_dataset,\n",
    "    compute_pca_embeddings,\n",
    "    get_dataset_and_loader\n",
    ")\n",
    "\n",
    "# Load datasets with fixed transforms for PCA\n",
    "print(\"Loading Imagenette...\")\n",
    "train_dataset_fixed = get_imagenette_dataset(\n",
    "    root=CONFIG['data_root'],\n",
    "    split='train',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    fixed_transform=True\n",
    ")\n",
    "\n",
    "val_dataset_fixed = get_imagenette_dataset(\n",
    "    root=CONFIG['data_root'],\n",
    "    split='val',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    fixed_transform=True\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset_fixed)}, Val: {len(val_dataset_fixed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA embeddings\n",
    "os.makedirs('./embeddings', exist_ok=True)\n",
    "\n",
    "print(\"Computing PCA embeddings...\")\n",
    "train_pca = compute_pca_embeddings(\n",
    "    train_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=64\n",
    ")\n",
    "torch.save(train_pca, './embeddings/train_pca.pt')\n",
    "\n",
    "val_pca = compute_pca_embeddings(\n",
    "    val_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=64\n",
    ")\n",
    "torch.save(val_pca, './embeddings/val_pca.pt')\n",
    "\n",
    "print(f\"Train PCA: {train_pca.shape}, Val PCA: {val_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train VAEs\n",
    "\n",
    "### 3.1 Baseline VAE (No MM-Reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.vae_wrapper import load_vae\n",
    "from src.models.losses import VAELoss\n",
    "from src.trainer import MMRegTrainer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING BASELINE VAE (no MM-Reg)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load fresh VAE\n",
    "vae_baseline = load_vae(device=device)\n",
    "\n",
    "# Loss without MM-Reg (lambda_mm=0)\n",
    "loss_baseline = VAELoss(lambda_mm=0.0, beta=CONFIG['beta'])\n",
    "\n",
    "# Data loaders (without PCA embeddings for baseline)\n",
    "train_dataset_base, train_loader_base = get_dataset_and_loader(\n",
    "    dataset_name='imagenette',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='train',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/train_pca.pt'  # Still need for trainer compatibility\n",
    ")\n",
    "\n",
    "val_dataset_base, val_loader_base = get_dataset_and_loader(\n",
    "    dataset_name='imagenette',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='val',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/val_pca.pt'\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer_baseline = torch.optim.AdamW(vae_baseline.parameters(), lr=CONFIG['vae_lr'])\n",
    "\n",
    "# Trainer\n",
    "trainer_baseline = MMRegTrainer(\n",
    "    vae=vae_baseline,\n",
    "    loss_fn=loss_baseline,\n",
    "    optimizer=optimizer_baseline,\n",
    "    train_loader=train_loader_base,\n",
    "    val_loader=val_loader_base,\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/baseline_vae'\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer_baseline.train(num_epochs=CONFIG['vae_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MM-Reg VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING MM-REG VAE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load fresh VAE\n",
    "vae_mmreg = load_vae(device=device)\n",
    "\n",
    "# Loss with MM-Reg\n",
    "loss_mmreg = VAELoss(\n",
    "    lambda_mm=CONFIG['lambda_mm'],\n",
    "    beta=CONFIG['beta'],\n",
    "    mm_variant='correlation'\n",
    ")\n",
    "\n",
    "# Use same data loaders (they have PCA embeddings)\n",
    "train_dataset_mm, train_loader_mm = get_dataset_and_loader(\n",
    "    dataset_name='imagenette',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='train',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/train_pca.pt'\n",
    ")\n",
    "\n",
    "val_dataset_mm, val_loader_mm = get_dataset_and_loader(\n",
    "    dataset_name='imagenette',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='val',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/val_pca.pt'\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer_mmreg = torch.optim.AdamW(vae_mmreg.parameters(), lr=CONFIG['vae_lr'])\n",
    "\n",
    "# Trainer\n",
    "trainer_mmreg = MMRegTrainer(\n",
    "    vae=vae_mmreg,\n",
    "    loss_fn=loss_mmreg,\n",
    "    optimizer=optimizer_mmreg,\n",
    "    train_loader=train_loader_mm,\n",
    "    val_loader=val_loader_mm,\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/mmreg_vae'\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer_mmreg.train(num_epochs=CONFIG['vae_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.losses import pairwise_distances, get_upper_triangular\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def evaluate_vae(vae, val_loader, name):\n",
    "    \"\"\"Evaluate VAE: reconstruction + distance correlation.\"\"\"\n",
    "    vae.eval()\n",
    "    \n",
    "    all_latents = []\n",
    "    all_pca = []\n",
    "    total_recon_error = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, _, pca_emb = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            outputs = vae(images, sample=False)\n",
    "            \n",
    "            # Reconstruction error\n",
    "            recon_error = ((outputs['x_recon'] - images) ** 2).mean().item()\n",
    "            total_recon_error += recon_error * images.shape[0]\n",
    "            num_samples += images.shape[0]\n",
    "            \n",
    "            all_latents.append(outputs['latent_flat'].cpu())\n",
    "            all_pca.append(pca_emb)\n",
    "    \n",
    "    # Concatenate\n",
    "    all_latents = torch.cat(all_latents, dim=0)\n",
    "    all_pca = torch.cat(all_pca, dim=0)\n",
    "    \n",
    "    # Distance correlation (use subset for speed)\n",
    "    n = min(500, len(all_latents))\n",
    "    D_latent = pairwise_distances(all_latents[:n])\n",
    "    D_pca = pairwise_distances(all_pca[:n])\n",
    "    \n",
    "    d_latent = get_upper_triangular(D_latent).numpy()\n",
    "    d_pca = get_upper_triangular(D_pca).numpy()\n",
    "    \n",
    "    pearson, _ = pearsonr(d_latent, d_pca)\n",
    "    spearman, _ = spearmanr(d_latent, d_pca)\n",
    "    \n",
    "    results = {\n",
    "        'recon_mse': total_recon_error / num_samples,\n",
    "        'pearson_corr': pearson,\n",
    "        'spearman_corr': spearman\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"  Reconstruction MSE: {results['recon_mse']:.6f}\")\n",
    "    print(f\"  Distance Pearson:   {results['pearson_corr']:.4f}\")\n",
    "    print(f\"  Distance Spearman:  {results['spearman_corr']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate both VAEs\n",
    "results_baseline = evaluate_vae(vae_baseline, val_loader_base, \"Baseline VAE\")\n",
    "results_mmreg = evaluate_vae(vae_mmreg, val_loader_mm, \"MM-Reg VAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "def plot_reconstructions(vae, val_loader, title, save_path):\n",
    "    vae.eval()\n",
    "    batch = next(iter(val_loader))\n",
    "    images = batch[0][:8].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = vae(images, sample=False)\n",
    "        recon = outputs['x_recon']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    \n",
    "    for i in range(8):\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = ((img + 1) / 2).clip(0, 1)\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        rec = recon[i].cpu().permute(1, 2, 0).numpy()\n",
    "        rec = ((rec + 1) / 2).clip(0, 1)\n",
    "        axes[1, i].imshow(rec)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Recon', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_reconstructions(vae_baseline, val_loader_base, \"Baseline VAE Reconstructions\", \n",
    "                     \"./checkpoints/baseline_vae/reconstructions.png\")\n",
    "plot_reconstructions(vae_mmreg, val_loader_mm, \"MM-Reg VAE Reconstructions\",\n",
    "                     \"./checkpoints/mmreg_vae/reconstructions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Diffusion Models\n",
    "\n",
    "### 5.1 Encode Datasets to Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.diffusion_trainer import encode_dataset\n",
    "\n",
    "# Create simple dataloaders without PCA for encoding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader_simple = DataLoader(\n",
    "    train_dataset_fixed,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader_simple = DataLoader(\n",
    "    val_dataset_fixed,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Encode with baseline VAE\n",
    "print(\"Encoding dataset with Baseline VAE...\")\n",
    "train_latents_baseline = encode_dataset(vae_baseline, train_loader_simple, device)\n",
    "val_latents_baseline = encode_dataset(vae_baseline, val_loader_simple, device)\n",
    "print(f\"Baseline latents - Train: {train_latents_baseline.shape}, Val: {val_latents_baseline.shape}\")\n",
    "\n",
    "# Encode with MM-Reg VAE\n",
    "print(\"\\nEncoding dataset with MM-Reg VAE...\")\n",
    "train_latents_mmreg = encode_dataset(vae_mmreg, train_loader_simple, device)\n",
    "val_latents_mmreg = encode_dataset(vae_mmreg, val_loader_simple, device)\n",
    "print(f\"MM-Reg latents - Train: {train_latents_mmreg.shape}, Val: {val_latents_mmreg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train Diffusion on Baseline Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.diffusion import SimpleUNet, GaussianDiffusion\n",
    "from src.diffusion_trainer import DiffusionTrainer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING DIFFUSION ON BASELINE LATENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create diffusion model\n",
    "diffusion_baseline = GaussianDiffusion(\n",
    "    num_timesteps=CONFIG['diffusion_timesteps'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "unet_baseline = SimpleUNet(\n",
    "    in_channels=4,\n",
    "    base_channels=128,\n",
    "    channel_mult=(1, 2, 4),\n",
    "    num_res_blocks=2\n",
    ").to(device)\n",
    "\n",
    "optimizer_diff_base = torch.optim.AdamW(unet_baseline.parameters(), lr=CONFIG['diffusion_lr'])\n",
    "\n",
    "trainer_diff_baseline = DiffusionTrainer(\n",
    "    model=unet_baseline,\n",
    "    diffusion=diffusion_baseline,\n",
    "    optimizer=optimizer_diff_base,\n",
    "    train_latents=train_latents_baseline,\n",
    "    val_latents=val_latents_baseline,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/diffusion_baseline'\n",
    ")\n",
    "\n",
    "trainer_diff_baseline.train(num_epochs=CONFIG['diffusion_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train Diffusion on MM-Reg Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING DIFFUSION ON MM-REG LATENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create diffusion model\n",
    "diffusion_mmreg = GaussianDiffusion(\n",
    "    num_timesteps=CONFIG['diffusion_timesteps'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "unet_mmreg = SimpleUNet(\n",
    "    in_channels=4,\n",
    "    base_channels=128,\n",
    "    channel_mult=(1, 2, 4),\n",
    "    num_res_blocks=2\n",
    ").to(device)\n",
    "\n",
    "optimizer_diff_mm = torch.optim.AdamW(unet_mmreg.parameters(), lr=CONFIG['diffusion_lr'])\n",
    "\n",
    "trainer_diff_mmreg = DiffusionTrainer(\n",
    "    model=unet_mmreg,\n",
    "    diffusion=diffusion_mmreg,\n",
    "    optimizer=optimizer_diff_mm,\n",
    "    train_latents=train_latents_mmreg,\n",
    "    val_latents=val_latents_mmreg,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/diffusion_mmreg'\n",
    ")\n",
    "\n",
    "trainer_diff_mmreg.train(num_epochs=CONFIG['diffusion_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Samples & Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from both diffusion models\n",
    "print(\"Generating samples from Baseline Diffusion...\")\n",
    "samples_baseline = trainer_diff_baseline.generate_samples(num_samples=16)\n",
    "\n",
    "print(\"\\nGenerating samples from MM-Reg Diffusion...\")\n",
    "samples_mmreg = trainer_diff_mmreg.generate_samples(num_samples=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode latents to images\n",
    "def decode_and_plot(vae, latents, title, save_path):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        latents = latents.to(device)\n",
    "        images = vae.decode(latents)\n",
    "    \n",
    "    # Plot\n",
    "    n = min(16, images.shape[0])\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    \n",
    "    for i in range(n):\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = ((img + 1) / 2).clip(0, 1)\n",
    "        ax = axes[i // 8, i % 8]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "decode_and_plot(vae_baseline, samples_baseline, \n",
    "                \"Generated Samples (Baseline VAE + Diffusion)\",\n",
    "                \"./checkpoints/diffusion_baseline/generated_samples.png\")\n",
    "\n",
    "decode_and_plot(vae_mmreg, samples_mmreg,\n",
    "                \"Generated Samples (MM-Reg VAE + Diffusion)\",\n",
    "                \"./checkpoints/diffusion_mmreg/generated_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# VAE losses\n",
    "with open('./checkpoints/baseline_vae/history.json') as f:\n",
    "    baseline_vae_hist = json.load(f)\n",
    "with open('./checkpoints/mmreg_vae/history.json') as f:\n",
    "    mmreg_vae_hist = json.load(f)\n",
    "\n",
    "axes[0].plot([h['loss'] for h in baseline_vae_hist['train']], 'b-', label='Baseline Train')\n",
    "axes[0].plot([h['loss'] for h in baseline_vae_hist['val']], 'b--', label='Baseline Val')\n",
    "axes[0].plot([h['loss'] for h in mmreg_vae_hist['train']], 'r-', label='MM-Reg Train')\n",
    "axes[0].plot([h['loss'] for h in mmreg_vae_hist['val']], 'r--', label='MM-Reg Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('VAE Training')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Diffusion losses\n",
    "with open('./checkpoints/diffusion_baseline/history.json') as f:\n",
    "    baseline_diff_hist = json.load(f)\n",
    "with open('./checkpoints/diffusion_mmreg/history.json') as f:\n",
    "    mmreg_diff_hist = json.load(f)\n",
    "\n",
    "axes[1].plot([h['loss'] for h in baseline_diff_hist['train']], 'b-', label='Baseline Train')\n",
    "axes[1].plot([h['loss'] for h in baseline_diff_hist['val']], 'b--', label='Baseline Val')\n",
    "axes[1].plot([h['loss'] for h in mmreg_diff_hist['train']], 'r-', label='MM-Reg Train')\n",
    "axes[1].plot([h['loss'] for h in mmreg_diff_hist['val']], 'r--', label='MM-Reg Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Diffusion Training')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./checkpoints/training_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = {\n",
    "    'config': CONFIG,\n",
    "    'vae_results': {\n",
    "        'baseline': results_baseline,\n",
    "        'mmreg': results_mmreg\n",
    "    },\n",
    "    'diffusion_final_loss': {\n",
    "        'baseline_train': baseline_diff_hist['train'][-1]['loss'],\n",
    "        'baseline_val': baseline_diff_hist['val'][-1]['loss'],\n",
    "        'mmreg_train': mmreg_diff_hist['train'][-1]['loss'],\n",
    "        'mmreg_val': mmreg_diff_hist['val'][-1]['loss']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nVAE Comparison:\")\n",
    "print(f\"  Baseline - Recon MSE: {results_baseline['recon_mse']:.6f}, Pearson: {results_baseline['pearson_corr']:.4f}\")\n",
    "print(f\"  MM-Reg   - Recon MSE: {results_mmreg['recon_mse']:.6f}, Pearson: {results_mmreg['pearson_corr']:.4f}\")\n",
    "\n",
    "print(\"\\nDiffusion Final Val Loss:\")\n",
    "print(f\"  Baseline: {summary['diffusion_final_loss']['baseline_val']:.6f}\")\n",
    "print(f\"  MM-Reg:   {summary['diffusion_final_loss']['mmreg_val']:.6f}\")\n",
    "\n",
    "# Save summary\n",
    "with open('./checkpoints/experiment_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to ./checkpoints/experiment_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
