{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# MM-Reg: Dataset Scaling Experiment on CelebA\n",
    "\n",
    "Does MM-Reg's diffusion advantage grow, shrink, or stay constant as dataset size increases?\n",
    "\n",
    "## Design:\n",
    "1. Fine-tune TWO SD VAEs once on full CelebA (~160k): **Baseline** vs **MM-Reg**\n",
    "2. Encode all images with both VAEs (cached)\n",
    "3. For each dataset scale (10k, 20k, 50k, 100k, 160k):\n",
    "   - Subsample cached latents\n",
    "   - Train diffusion on baseline latents (50 epochs)\n",
    "   - Train diffusion on MM-Reg latents (50 epochs)\n",
    "4. Compare diffusion val loss across scales\n",
    "\n",
    "**Validation set is fixed** across all scales for fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf MMReg_diffusion_generative 2>/dev/null\n",
    "!git clone https://github.com/laurent-cheret/MMReg_diffusion_generative.git\n",
    "%cd MMReg_diffusion_generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision diffusers transformers accelerate\n",
    "!pip install -q pyyaml tqdm scipy scikit-learn matplotlib\n",
    "!pip install -q datasets gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_root': './data',\n",
    "    'image_size': 128,\n",
    "    'batch_size': 64,\n",
    "\n",
    "    # PCA\n",
    "    'pca_components': 256,\n",
    "\n",
    "    # VAE Training (on full dataset, once)\n",
    "    'vae_epochs': 5,\n",
    "    'vae_lr': 1e-5,\n",
    "    'lambda_mm': 1.0,\n",
    "    'beta': 1e-6,\n",
    "\n",
    "    # Diffusion Training (per scale)\n",
    "    'diffusion_epochs': 50,\n",
    "    'diffusion_lr': 1e-4,\n",
    "    'diffusion_timesteps': 1000,\n",
    "\n",
    "    # Dataset scales to test\n",
    "    'scales': [10000, 20000, 50000, 100000, 160000],\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Load CelebA & Pre-compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataset import (\n",
    "    get_celeba_dataset,\n",
    "    compute_pca_embeddings_celeba,\n",
    "    get_dataset_and_loader\n",
    ")\n",
    "\n",
    "# === CONFIGURATION: Choose your data source ===\n",
    "# Option 1: HuggingFace (recommended)\n",
    "USE_HUGGINGFACE = True\n",
    "\n",
    "# Option 2: Google Drive\n",
    "USE_DRIVE = False\n",
    "DRIVE_PATHS = {\n",
    "    'images_zip': '/content/drive/MyDrive/DATASETS/CelebA/img_align_celeba.zip',\n",
    "    'attr_file': '/content/drive/MyDrive/DATASETS/CelebA/list_attr_celeba.txt',\n",
    "    'partition_file': '/content/drive/MyDrive/DATASETS/CelebA/list_eval_partition.txt'\n",
    "}\n",
    "\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "source = 'drive' if USE_DRIVE else 'huggingface'\n",
    "drive_paths = DRIVE_PATHS if USE_DRIVE else None\n",
    "\n",
    "# Load full CelebA with fixed transforms for PCA\n",
    "print(f\"Loading CelebA from {source}...\")\n",
    "train_dataset_fixed = get_celeba_dataset(\n",
    "    root=CONFIG['data_root'],\n",
    "    split='train',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    fixed_transform=True,\n",
    "    source=source,\n",
    "    drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "val_dataset_fixed = get_celeba_dataset(\n",
    "    root=CONFIG['data_root'],\n",
    "    split='val',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    fixed_transform=True,\n",
    "    source=source,\n",
    "    drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "total_train = len(train_dataset_fixed)\n",
    "print(f\"Train: {total_train}, Val: {len(val_dataset_fixed)}\")\n",
    "\n",
    "# Cap scales at actual dataset size\n",
    "CONFIG['scales'] = [s for s in CONFIG['scales'] if s <= total_train]\n",
    "if total_train not in CONFIG['scales']:\n",
    "    CONFIG['scales'].append(total_train)\n",
    "print(f\"Scales to test: {CONFIG['scales']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA embeddings on full training set\n",
    "os.makedirs('./embeddings', exist_ok=True)\n",
    "\n",
    "print(\"Computing PCA embeddings for training set...\")\n",
    "train_pca = compute_pca_embeddings_celeba(\n",
    "    train_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=256\n",
    ")\n",
    "torch.save(train_pca, './embeddings/celeba_train_pca.pt')\n",
    "\n",
    "print(\"Computing PCA embeddings for validation set...\")\n",
    "val_pca = compute_pca_embeddings_celeba(\n",
    "    val_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=256\n",
    ")\n",
    "torch.save(val_pca, './embeddings/celeba_val_pca.pt')\n",
    "\n",
    "print(f\"Train PCA: {train_pca.shape}, Val PCA: {val_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Fine-tune VAEs on Full CelebA\n",
    "\n",
    "Train two SD VAEs once on the full dataset. These are reused across all scales.\n",
    "\n",
    "### 3.1 Baseline VAE (no MM-Reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.vae_wrapper import load_vae\n",
    "from src.models.losses import VAELoss\n",
    "from src.trainer import MMRegTrainer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING BASELINE VAE (no MM-Reg) on full CelebA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vae_baseline = load_vae(device=device)\n",
    "\n",
    "loss_baseline = VAELoss(lambda_mm=0.0, beta=CONFIG['beta'])\n",
    "\n",
    "train_dataset_base, train_loader_base = get_dataset_and_loader(\n",
    "    dataset_name='celeba',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='train',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/celeba_train_pca.pt',\n",
    "    celeba_source=source,\n",
    "    celeba_drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "val_dataset_base, val_loader_base = get_dataset_and_loader(\n",
    "    dataset_name='celeba',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='val',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/celeba_val_pca.pt',\n",
    "    celeba_source=source,\n",
    "    celeba_drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "optimizer_baseline = torch.optim.AdamW(vae_baseline.parameters(), lr=CONFIG['vae_lr'])\n",
    "\n",
    "trainer_baseline = MMRegTrainer(\n",
    "    vae=vae_baseline,\n",
    "    loss_fn=loss_baseline,\n",
    "    optimizer=optimizer_baseline,\n",
    "    train_loader=train_loader_base,\n",
    "    val_loader=val_loader_base,\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/scaling_baseline_vae'\n",
    ")\n",
    "\n",
    "trainer_baseline.train(num_epochs=CONFIG['vae_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 3.2 MM-Reg VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING MM-REG VAE on full CelebA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vae_mmreg = load_vae(device=device)\n",
    "\n",
    "loss_mmreg = VAELoss(\n",
    "    lambda_mm=CONFIG['lambda_mm'],\n",
    "    beta=CONFIG['beta'],\n",
    "    mm_variant='correlation'\n",
    ")\n",
    "\n",
    "train_dataset_mm, train_loader_mm = get_dataset_and_loader(\n",
    "    dataset_name='celeba',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='train',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/celeba_train_pca.pt',\n",
    "    celeba_source=source,\n",
    "    celeba_drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "val_dataset_mm, val_loader_mm = get_dataset_and_loader(\n",
    "    dataset_name='celeba',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='val',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/celeba_val_pca.pt',\n",
    "    celeba_source=source,\n",
    "    celeba_drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "optimizer_mmreg = torch.optim.AdamW(vae_mmreg.parameters(), lr=CONFIG['vae_lr'])\n",
    "\n",
    "trainer_mmreg = MMRegTrainer(\n",
    "    vae=vae_mmreg,\n",
    "    loss_fn=loss_mmreg,\n",
    "    optimizer=optimizer_mmreg,\n",
    "    train_loader=train_loader_mm,\n",
    "    val_loader=val_loader_mm,\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/scaling_mmreg_vae'\n",
    ")\n",
    "\n",
    "trainer_mmreg.train(num_epochs=CONFIG['vae_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Evaluate VAEs & Encode Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.losses import pairwise_distances, get_upper_triangular\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def quick_evaluate(vae, val_loader, val_pca, name):\n",
    "    \"\"\"Quick VAE evaluation: recon MSE + Pearson correlation.\"\"\"\n",
    "    vae.eval()\n",
    "    all_latents = []\n",
    "    total_recon = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Eval {name}\"):\n",
    "            images = batch[0].to(device)\n",
    "            outputs = vae(images, sample=False)\n",
    "            total_recon += ((outputs['x_recon'] - images) ** 2).mean().item() * images.shape[0]\n",
    "            num_samples += images.shape[0]\n",
    "            all_latents.append(outputs['latent_flat'].cpu())\n",
    "\n",
    "    all_latents = torch.cat(all_latents, dim=0)\n",
    "\n",
    "    n = min(500, len(all_latents))\n",
    "    D_lat = pairwise_distances(all_latents[:n])\n",
    "    D_pca = pairwise_distances(val_pca[:n])\n",
    "    d_lat = get_upper_triangular(D_lat).numpy()\n",
    "    d_pca = get_upper_triangular(D_pca).numpy()\n",
    "    pearson, _ = pearsonr(d_lat, d_pca)\n",
    "\n",
    "    recon_mse = total_recon / num_samples\n",
    "    print(f\"{name}: Recon MSE={recon_mse:.6f}, Pearson={pearson:.4f}\")\n",
    "    return {'recon_mse': recon_mse, 'pearson': pearson}\n",
    "\n",
    "print(\"\\nVAE Evaluation (on full val set):\")\n",
    "vae_results_baseline = quick_evaluate(vae_baseline, val_loader_base, val_pca, \"Baseline\")\n",
    "vae_results_mmreg = quick_evaluate(vae_mmreg, val_loader_mm, val_pca, \"MM-Reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.diffusion_trainer import encode_dataset\n",
    "\n",
    "# Simple dataloaders for encoding (no PCA wrapper)\n",
    "train_loader_simple = DataLoader(\n",
    "    train_dataset_fixed,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "val_loader_simple = DataLoader(\n",
    "    val_dataset_fixed,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Encode full dataset with both VAEs\n",
    "print(\"Encoding full dataset with Baseline VAE...\")\n",
    "train_latents_baseline = encode_dataset(vae_baseline, train_loader_simple, device)\n",
    "val_latents_baseline = encode_dataset(vae_baseline, val_loader_simple, device)\n",
    "print(f\"Baseline latents - Train: {train_latents_baseline.shape}, Val: {val_latents_baseline.shape}\")\n",
    "\n",
    "print(\"\\nEncoding full dataset with MM-Reg VAE...\")\n",
    "train_latents_mmreg = encode_dataset(vae_mmreg, train_loader_simple, device)\n",
    "val_latents_mmreg = encode_dataset(vae_mmreg, val_loader_simple, device)\n",
    "print(f\"MM-Reg latents - Train: {train_latents_mmreg.shape}, Val: {val_latents_mmreg.shape}\")\n",
    "\n",
    "# Save for potential reuse\n",
    "torch.save(train_latents_baseline, './embeddings/scaling_train_latents_baseline.pt')\n",
    "torch.save(val_latents_baseline, './embeddings/scaling_val_latents_baseline.pt')\n",
    "torch.save(train_latents_mmreg, './embeddings/scaling_train_latents_mmreg.pt')\n",
    "torch.save(val_latents_mmreg, './embeddings/scaling_val_latents_mmreg.pt')\n",
    "print(\"\\nLatents cached to ./embeddings/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Scaling Experiment: Diffusion Training at Each Scale\n",
    "\n",
    "For each dataset size, subsample training latents and train fresh diffusion models.\n",
    "Validation set stays fixed across all scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "from src.models.diffusion import SimpleUNet, GaussianDiffusion\nfrom src.diffusion_trainer import DiffusionTrainer\nimport copy\n\n# Latent spatial size for UNet\nlatent_size = train_latents_baseline.shape[2]  # 16 for 128x128 images\nprint(f\"Latent size: {latent_size}x{latent_size}\")\n\n# === NORMALIZE LATENTS ===\n# Diffusion assumes data is roughly N(0,1). Raw VAE latents are not!\nprint(\"\\nNormalizing latents...\")\nbaseline_mean = train_latents_baseline.mean()\nbaseline_std = train_latents_baseline.std()\nmmreg_mean = train_latents_mmreg.mean()\nmmreg_std = train_latents_mmreg.std()\n\nprint(f\"Baseline latents - mean: {baseline_mean:.4f}, std: {baseline_std:.4f}\")\nprint(f\"MM-Reg latents   - mean: {mmreg_mean:.4f}, std: {mmreg_std:.4f}\")\n\ntrain_latents_baseline_norm = (train_latents_baseline - baseline_mean) / baseline_std\nval_latents_baseline_norm = (val_latents_baseline - baseline_mean) / baseline_std\ntrain_latents_mmreg_norm = (train_latents_mmreg - mmreg_mean) / mmreg_std\nval_latents_mmreg_norm = (val_latents_mmreg - mmreg_mean) / mmreg_std\n\n# Save normalization stats for generation\nlatent_stats = {\n    'baseline': {'mean': baseline_mean.item(), 'std': baseline_std.item()},\n    'mmreg': {'mean': mmreg_mean.item(), 'std': mmreg_std.item()}\n}\ntorch.save(latent_stats, './embeddings/latent_stats.pt')\nprint(\"Latent stats saved for denormalization during generation\")\n\n# === EMA HELPER ===\ndef update_ema(ema_model, model, decay=0.9999):\n    with torch.no_grad():\n        for ema_p, p in zip(ema_model.parameters(), model.parameters()):\n            ema_p.data.mul_(decay).add_(p.data, alpha=1 - decay)\n\n# Store results\nscaling_results = {}\n\nfor scale in CONFIG['scales']:\n    # Cap at actual dataset size\n    n = min(scale, len(train_latents_baseline_norm))\n    print(f\"\\n{'='*60}\")\n    print(f\"SCALE: {n:,} training samples\")\n    print(f\"{'='*60}\")\n\n    # Subsample training latents (first N, deterministic)\n    sub_train_baseline = train_latents_baseline_norm[:n]\n    sub_train_mmreg = train_latents_mmreg_norm[:n]\n\n    # --- Baseline Diffusion ---\n    print(f\"\\n--- Baseline Diffusion ({n:,} samples) ---\")\n\n    diffusion_base = GaussianDiffusion(\n        num_timesteps=CONFIG['diffusion_timesteps'],\n        device=device\n    )\n\n    unet_base = SimpleUNet(\n        in_channels=4,\n        base_channels=128,\n        channel_mult=(1, 2, 4),\n        num_res_blocks=2\n    ).to(device)\n    \n    # EMA model\n    ema_base = copy.deepcopy(unet_base)\n    for p in ema_base.parameters():\n        p.requires_grad = False\n\n    opt_base = torch.optim.AdamW(unet_base.parameters(), lr=CONFIG['diffusion_lr'])\n    \n    # Cosine LR schedule\n    scheduler_base = torch.optim.lr_scheduler.CosineAnnealingLR(\n        opt_base, T_max=CONFIG['diffusion_epochs']\n    )\n\n    trainer_base = DiffusionTrainer(\n        model=unet_base,\n        diffusion=diffusion_base,\n        optimizer=opt_base,\n        train_latents=sub_train_baseline,\n        val_latents=val_latents_baseline_norm,\n        batch_size=CONFIG['batch_size'],\n        device=device,\n        save_dir=f'./checkpoints/scaling_diff_baseline_{n}',\n        scheduler=scheduler_base\n    )\n\n    # Custom training loop with gradient clipping and EMA\n    print(f\"Training diffusion for {CONFIG['diffusion_epochs']} epochs...\")\n    for epoch in range(CONFIG['diffusion_epochs']):\n        trainer_base.model.train()\n        total_loss = 0\n        num_batches = 0\n        \n        for latents in trainer_base.train_loader:\n            latents = latents.to(device)\n            batch_size = latents.shape[0]\n            t = torch.randint(0, diffusion_base.num_timesteps, (batch_size,), device=device)\n            \n            opt_base.zero_grad()\n            loss = diffusion_base.p_losses(unet_base, latents, t)\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(unet_base.parameters(), max_norm=1.0)\n            \n            opt_base.step()\n            \n            # Update EMA\n            update_ema(ema_base, unet_base)\n            \n            total_loss += loss.item()\n            num_batches += 1\n        \n        scheduler_base.step()\n        \n        # Validation\n        val_loss = trainer_base.validate()['loss']\n        trainer_base.train_history.append({'loss': total_loss / num_batches})\n        trainer_base.val_history.append({'loss': val_loss})\n        \n        if epoch % 10 == 0 or epoch == CONFIG['diffusion_epochs'] - 1:\n            print(f\"  Epoch {epoch}: train={total_loss/num_batches:.6f}, val={val_loss:.6f}\")\n    \n    # Save EMA model as best\n    torch.save({'model_state_dict': ema_base.state_dict()}, \n               f'./checkpoints/scaling_diff_baseline_{n}/best.pt')\n\n    # --- MM-Reg Diffusion ---\n    print(f\"\\n--- MM-Reg Diffusion ({n:,} samples) ---\")\n\n    diffusion_mm = GaussianDiffusion(\n        num_timesteps=CONFIG['diffusion_timesteps'],\n        device=device\n    )\n\n    unet_mm = SimpleUNet(\n        in_channels=4,\n        base_channels=128,\n        channel_mult=(1, 2, 4),\n        num_res_blocks=2\n    ).to(device)\n    \n    # EMA model\n    ema_mm = copy.deepcopy(unet_mm)\n    for p in ema_mm.parameters():\n        p.requires_grad = False\n\n    opt_mm = torch.optim.AdamW(unet_mm.parameters(), lr=CONFIG['diffusion_lr'])\n    \n    scheduler_mm = torch.optim.lr_scheduler.CosineAnnealingLR(\n        opt_mm, T_max=CONFIG['diffusion_epochs']\n    )\n\n    trainer_mm = DiffusionTrainer(\n        model=unet_mm,\n        diffusion=diffusion_mm,\n        optimizer=opt_mm,\n        train_latents=sub_train_mmreg,\n        val_latents=val_latents_mmreg_norm,\n        batch_size=CONFIG['batch_size'],\n        device=device,\n        save_dir=f'./checkpoints/scaling_diff_mmreg_{n}',\n        scheduler=scheduler_mm\n    )\n\n    print(f\"Training diffusion for {CONFIG['diffusion_epochs']} epochs...\")\n    for epoch in range(CONFIG['diffusion_epochs']):\n        trainer_mm.model.train()\n        total_loss = 0\n        num_batches = 0\n        \n        for latents in trainer_mm.train_loader:\n            latents = latents.to(device)\n            batch_size = latents.shape[0]\n            t = torch.randint(0, diffusion_mm.num_timesteps, (batch_size,), device=device)\n            \n            opt_mm.zero_grad()\n            loss = diffusion_mm.p_losses(unet_mm, latents, t)\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(unet_mm.parameters(), max_norm=1.0)\n            \n            opt_mm.step()\n            \n            # Update EMA\n            update_ema(ema_mm, unet_mm)\n            \n            total_loss += loss.item()\n            num_batches += 1\n        \n        scheduler_mm.step()\n        \n        # Validation\n        val_loss = trainer_mm.validate()['loss']\n        trainer_mm.train_history.append({'loss': total_loss / num_batches})\n        trainer_mm.val_history.append({'loss': val_loss})\n        \n        if epoch % 10 == 0 or epoch == CONFIG['diffusion_epochs'] - 1:\n            print(f\"  Epoch {epoch}: train={total_loss/num_batches:.6f}, val={val_loss:.6f}\")\n    \n    # Save EMA model as best\n    torch.save({'model_state_dict': ema_mm.state_dict()}, \n               f'./checkpoints/scaling_diff_mmreg_{n}/best.pt')\n\n    # Record results\n    base_val = trainer_base.val_history[-1]['loss']\n    mm_val = trainer_mm.val_history[-1]['loss']\n    improvement = (base_val - mm_val) / base_val * 100\n\n    scaling_results[n] = {\n        'baseline_train': trainer_base.train_history[-1]['loss'],\n        'baseline_val': base_val,\n        'mmreg_train': trainer_mm.train_history[-1]['loss'],\n        'mmreg_val': mm_val,\n        'improvement_pct': improvement,\n        'baseline_history': trainer_base.val_history,\n        'mmreg_history': trainer_mm.val_history,\n    }\n\n    print(f\"\\nScale {n:,}: Baseline val={base_val:.6f}, MM-Reg val={mm_val:.6f}, Improvement={improvement:.1f}%\")\n\n    # Free GPU memory\n    del unet_base, unet_mm, ema_base, ema_mm, trainer_base, trainer_mm\n    del diffusion_base, diffusion_mm, opt_base, opt_mm\n    torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ALL SCALES COMPLETE\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "print(f\"{'Scale':>10} | {'Baseline Val':>14} | {'MM-Reg Val':>14} | {'Improvement':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for n in sorted(scaling_results.keys()):\n",
    "    r = scaling_results[n]\n",
    "    print(f\"{n:>10,} | {r['baseline_val']:>14.6f} | {r['mmreg_val']:>14.6f} | {r['improvement_pct']:>11.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Val loss vs dataset size\n",
    "scales = sorted(scaling_results.keys())\n",
    "base_vals = [scaling_results[n]['baseline_val'] for n in scales]\n",
    "mm_vals = [scaling_results[n]['mmreg_val'] for n in scales]\n",
    "improvements = [scaling_results[n]['improvement_pct'] for n in scales]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: absolute val loss\n",
    "axes[0].plot(scales, base_vals, 'bo-', label='Baseline', markersize=8)\n",
    "axes[0].plot(scales, mm_vals, 'rs-', label='MM-Reg', markersize=8)\n",
    "axes[0].set_xlabel('Training Set Size', fontsize=12)\n",
    "axes[0].set_ylabel('Diffusion Val Loss', fontsize=12)\n",
    "axes[0].set_title('Diffusion Val Loss vs Dataset Size', fontsize=13)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xticks(scales)\n",
    "axes[0].set_xticklabels([f'{s//1000}k' for s in scales])\n",
    "\n",
    "# Right: improvement %\n",
    "axes[1].bar(range(len(scales)), improvements, color='green', alpha=0.7)\n",
    "axes[1].set_xlabel('Training Set Size', fontsize=12)\n",
    "axes[1].set_ylabel('MM-Reg Improvement (%)', fontsize=12)\n",
    "axes[1].set_title('MM-Reg Advantage vs Dataset Size', fontsize=13)\n",
    "axes[1].set_xticks(range(len(scales)))\n",
    "axes[1].set_xticklabels([f'{s//1000}k' for s in scales])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(improvements):\n",
    "    axes[1].text(i, v + 0.5, f'{v:.1f}%', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./checkpoints/scaling_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Training curves at each scale\n",
    "n_scales = len(scales)\n",
    "fig, axes = plt.subplots(1, n_scales, figsize=(4 * n_scales, 4), sharey=True)\n",
    "\n",
    "if n_scales == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, n in enumerate(scales):\n",
    "    r = scaling_results[n]\n",
    "    base_curve = [h['loss'] for h in r['baseline_history']]\n",
    "    mm_curve = [h['loss'] for h in r['mmreg_history']]\n",
    "\n",
    "    axes[i].plot(base_curve, 'b-', label='Baseline', alpha=0.8)\n",
    "    axes[i].plot(mm_curve, 'r-', label='MM-Reg', alpha=0.8)\n",
    "    axes[i].set_title(f'{n//1000}k samples', fontsize=12)\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Val Loss')\n",
    "    axes[i].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Diffusion Val Loss Curves Across Scales', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./checkpoints/scaling_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# Generate sample faces at largest scale for visual comparison\nlargest_scale = max(scales)\n\n# Load latent normalization stats\nlatent_stats = torch.load('./embeddings/latent_stats.pt')\nprint(f\"Latent stats loaded:\")\nprint(f\"  Baseline: mean={latent_stats['baseline']['mean']:.4f}, std={latent_stats['baseline']['std']:.4f}\")\nprint(f\"  MM-Reg:   mean={latent_stats['mmreg']['mean']:.4f}, std={latent_stats['mmreg']['std']:.4f}\")\n\n# Reload best diffusion models from largest scale\nprint(f\"\\nLoading best models from {largest_scale//1000}k scale...\")\n\ndiffusion_gen = GaussianDiffusion(\n    num_timesteps=CONFIG['diffusion_timesteps'],\n    device=device\n)\n\n# Baseline\nunet_gen_base = SimpleUNet(in_channels=4, base_channels=128, channel_mult=(1, 2, 4), num_res_blocks=2).to(device)\nckpt_base = torch.load(f'./checkpoints/scaling_diff_baseline_{largest_scale}/best.pt', map_location=device)\nunet_gen_base.load_state_dict(ckpt_base['model_state_dict'])\n\n# MM-Reg\nunet_gen_mm = SimpleUNet(in_channels=4, base_channels=128, channel_mult=(1, 2, 4), num_res_blocks=2).to(device)\nckpt_mm = torch.load(f'./checkpoints/scaling_diff_mmreg_{largest_scale}/best.pt', map_location=device)\nunet_gen_mm.load_state_dict(ckpt_mm['model_state_dict'])\n\n# Generate (in normalized space)\nprint(\"Generating baseline samples...\")\nlatent_shape = (16, 4, latent_size, latent_size)\ngen_latents_base_norm = diffusion_gen.sample(unet_gen_base, latent_shape, progress=True)\n\nprint(\"Generating MM-Reg samples...\")\ngen_latents_mm_norm = diffusion_gen.sample(unet_gen_mm, latent_shape, progress=True)\n\n# === DENORMALIZE before decoding ===\ngen_latents_base = gen_latents_base_norm * latent_stats['baseline']['std'] + latent_stats['baseline']['mean']\ngen_latents_mm = gen_latents_mm_norm * latent_stats['mmreg']['std'] + latent_stats['mmreg']['mean']\n\nprint(f\"\\nGenerated latent stats (after denorm):\")\nprint(f\"  Baseline: mean={gen_latents_base.mean():.4f}, std={gen_latents_base.std():.4f}\")\nprint(f\"  MM-Reg:   mean={gen_latents_mm.mean():.4f}, std={gen_latents_mm.std():.4f}\")\n\n# Decode to images\ndef decode_and_plot(vae, latents, title, save_path):\n    vae.eval()\n    with torch.no_grad():\n        images = vae.decode(latents.to(device))\n\n    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n    fig.suptitle(title, fontsize=14)\n    for i in range(16):\n        img = images[i].cpu().permute(1, 2, 0).numpy()\n        img = ((img + 1) / 2).clip(0, 1)\n        axes[i // 8, i % 8].imshow(img)\n        axes[i // 8, i % 8].axis('off')\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n\ndecode_and_plot(vae_baseline, gen_latents_base,\n               f\"Generated Faces - Baseline ({largest_scale//1000}k)\",\n               './checkpoints/scaling_generated_baseline.png')\n\ndecode_and_plot(vae_mmreg, gen_latents_mm,\n               f\"Generated Faces - MM-Reg ({largest_scale//1000}k)\",\n               './checkpoints/scaling_generated_mmreg.png')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_python(val):\n",
    "    if hasattr(val, 'item'):\n",
    "        return val.item()\n",
    "    return float(val)\n",
    "\n",
    "summary = {\n",
    "    'config': CONFIG,\n",
    "    'vae_results': {\n",
    "        'baseline': {k: to_python(v) for k, v in vae_results_baseline.items()},\n",
    "        'mmreg': {k: to_python(v) for k, v in vae_results_mmreg.items()}\n",
    "    },\n",
    "    'scaling_results': {\n",
    "        str(n): {\n",
    "            'baseline_val': to_python(r['baseline_val']),\n",
    "            'mmreg_val': to_python(r['mmreg_val']),\n",
    "            'improvement_pct': to_python(r['improvement_pct'])\n",
    "        }\n",
    "        for n, r in scaling_results.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('./checkpoints/scaling_experiment_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SCALING EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nVAE (trained on full {total_train:,}):\")\n",
    "print(f\"  Baseline: Recon MSE={vae_results_baseline['recon_mse']:.6f}, Pearson={vae_results_baseline['pearson']:.4f}\")\n",
    "print(f\"  MM-Reg:   Recon MSE={vae_results_mmreg['recon_mse']:.6f}, Pearson={vae_results_mmreg['pearson']:.4f}\")\n",
    "\n",
    "print(f\"\\nDiffusion scaling ({CONFIG['diffusion_epochs']} epochs each):\")\n",
    "print(f\"{'Scale':>10} | {'Baseline':>10} | {'MM-Reg':>10} | {'Improvement':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for n in sorted(scaling_results.keys()):\n",
    "    r = scaling_results[n]\n",
    "    print(f\"{n:>10,} | {r['baseline_val']:>10.6f} | {r['mmreg_val']:>10.6f} | {r['improvement_pct']:>11.1f}%\")\n",
    "\n",
    "# Trend analysis\n",
    "imps = [scaling_results[n]['improvement_pct'] for n in sorted(scaling_results.keys())]\n",
    "if imps[-1] > imps[0]:\n",
    "    trend = \"WIDENING - MM-Reg advantage grows with data\"\n",
    "elif imps[-1] < imps[0] - 2:\n",
    "    trend = \"NARROWING - MM-Reg advantage decreases with data\"\n",
    "else:\n",
    "    trend = \"STABLE - MM-Reg advantage is consistent across scales\"\n",
    "\n",
    "print(f\"\\nTrend: {trend}\")\n",
    "print(f\"\\nResults saved to ./checkpoints/scaling_experiment_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}