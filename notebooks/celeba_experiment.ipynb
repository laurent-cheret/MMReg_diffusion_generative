{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# MM-Reg: CelebA Experiment with Bottleneck Architecture\n\nTwo-stage latent architecture for sharp reconstruction AND stable interpolation.\n\n## Architecture:\n```\nImage → SD VAE encoder → 4x16x16 (spatial) → BottleneckVAE → 256-d (semantic) → BottleneckVAE decoder → 4x16x16 → SD VAE decoder → Image\n```\n\n## Pipeline:\n1. **Setup**: Install dependencies, load CelebA\n2. **Pre-compute PCA**: Reference embeddings for MM-Reg\n3. **Fine-tune SD VAE**: One shared VAE on CelebA (reconstruction only)\n4. **Encode to SD latents**: Cache all images as 4x16x16 latents\n5. **Train Bottleneck VAEs**: Baseline (no MM-Reg) vs MM-Reg on 4x16x16 → 256-d\n6. **Attribute Directions**: Find directions in 256-d bottleneck space\n7. **Interpolation**: Test attribute manipulation in bottleneck space\n8. **Train Diffusion**: On 256-d bottleneck vectors\n9. **Generate + Interpolate**: Apply attributes to diffusion-generated faces\n\n**Key insight**: The 256-d bottleneck forces semantic compression (like the old flat VAE),\nwhile the frozen SD decoder provides sharp reconstruction quality."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf MMReg_diffusion_generative 2>/dev/null\n",
    "!git clone https://github.com/laurent-cheret/MMReg_diffusion_generative.git\n",
    "%cd MMReg_diffusion_generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch torchvision diffusers transformers accelerate\n!pip install -q pyyaml tqdm scipy scikit-learn matplotlib\n!pip install -q datasets  # For HuggingFace CelebA\n!pip install -q gdown  # For Google Drive fallback"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Experiment configuration\nCONFIG = {\n    # Data\n    'data_root': './data',\n    'image_size': 128,  # CelebA at 128x128\n    'batch_size': 64,\n    \n    # PCA\n    'pca_components': 256,\n    \n    # SD VAE fine-tuning (shared, reconstruction only)\n    'sd_vae_epochs': 5,\n    'sd_vae_lr': 1e-5,\n    \n    # Bottleneck VAE\n    'bottleneck_dim': 256,\n    'bottleneck_hidden': 512,\n    'bottleneck_epochs': 100,\n    'bottleneck_lr': 1e-3,\n    'bottleneck_batch_size': 256,\n    'lambda_mm': 1.0,\n    'beta': 0.001,\n    \n    # Diffusion Training (on 256-d bottleneck vectors)\n    'diffusion_epochs': 200,\n    'diffusion_lr': 1e-4,\n    'diffusion_timesteps': 1000,\n    \n    # Attributes to test for interpolation\n    'test_attributes': ['Smiling', 'Eyeglasses', 'Male', 'Young', 'Blond_Hair'],\n}\n\n# CelebA attribute names\nCELEBA_ATTRIBUTES = [\n    '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n    'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n    'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n    'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n    'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n    'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks',\n    'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n    'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'\n]\n\ndef get_attr_idx(name):\n    \"\"\"Get index of attribute by name.\"\"\"\n    return CELEBA_ATTRIBUTES.index(name)\n\nprint(\"Configuration:\")\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Load CelebA & Pre-compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "from src.data.dataset import (\n    get_celeba_dataset,\n    compute_pca_embeddings_celeba,\n    get_dataset_and_loader,\n    CELEBA_ATTRIBUTES\n)\n\n# === CONFIGURATION: Choose your data source ===\n# Option 1: HuggingFace (recommended, no rate limits)\nUSE_HUGGINGFACE = True\n\n# Option 2: Google Drive (if you have CelebA saved there)\n# Set these paths if you have CelebA in your Drive\nUSE_DRIVE = False\nDRIVE_PATHS = {\n    'images_zip': '/content/drive/MyDrive/DATASETS/CelebA/img_align_celeba.zip',\n    'attr_file': '/content/drive/MyDrive/DATASETS/CelebA/list_attr_celeba.txt',\n    'partition_file': '/content/drive/MyDrive/DATASETS/CelebA/list_eval_partition.txt'\n}\n\n# Mount Google Drive if using Drive source\nif USE_DRIVE:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n# Determine source\nif USE_DRIVE:\n    source = 'drive'\n    drive_paths = DRIVE_PATHS\nelse:\n    source = 'huggingface'\n    drive_paths = None\n\n# Load CelebA with fixed transforms for PCA\nprint(f\"Loading CelebA from {source}...\")\ntrain_dataset_fixed = get_celeba_dataset(\n    root=CONFIG['data_root'],\n    split='train',\n    image_size=CONFIG['image_size'],\n    fixed_transform=True,\n    source=source,\n    drive_paths=drive_paths\n)\n\nval_dataset_fixed = get_celeba_dataset(\n    root=CONFIG['data_root'],\n    split='val',\n    image_size=CONFIG['image_size'],\n    fixed_transform=True,\n    source=source,\n    drive_paths=drive_paths\n)\n\nprint(f\"Train: {len(train_dataset_fixed)}, Val: {len(val_dataset_fixed)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA embeddings\n",
    "os.makedirs('./embeddings', exist_ok=True)\n",
    "\n",
    "print(\"Computing PCA embeddings for training set...\")\n",
    "train_pca = compute_pca_embeddings_celeba(\n",
    "    train_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=64\n",
    ")\n",
    "torch.save(train_pca, './embeddings/celeba_train_pca.pt')\n",
    "\n",
    "print(\"\\nComputing PCA embeddings for validation set...\")\n",
    "val_pca = compute_pca_embeddings_celeba(\n",
    "    val_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=64\n",
    ")\n",
    "torch.save(val_pca, './embeddings/celeba_val_pca.pt')\n",
    "\n",
    "print(f\"\\nTrain PCA: {train_pca.shape}, Val PCA: {val_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## 3. Fine-tune Shared SD VAE\n\nTrain ONE SD VAE on CelebA for reconstruction only (no MM-Reg at this stage).\nThis gives us a CelebA-adapted encoder/decoder. The bottleneck VAEs will add the MM-Reg later."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "from src.models.vae_wrapper import load_vae\nfrom src.models.losses import VAELoss\nfrom src.trainer import MMRegTrainer\n\nprint(\"=\"*60)\nprint(\"FINE-TUNING SHARED SD VAE (reconstruction only)\")\nprint(\"=\"*60)\n\n# Load pretrained SD VAE\nsd_vae = load_vae(device=device)\n\n# Loss: reconstruction + small KL, NO MM-Reg\nloss_sdvae = VAELoss(lambda_mm=0.0, beta=CONFIG.get('beta', 1e-6))\n\n# Data loaders with PCA (needed by trainer interface, but lambda_mm=0 ignores it)\ntrain_dataset_pca, train_loader_pca = get_dataset_and_loader(\n    dataset_name='celeba',\n    root=CONFIG['data_root'],\n    split='train',\n    image_size=CONFIG['image_size'],\n    batch_size=CONFIG['batch_size'],\n    num_workers=2,\n    pca_embeddings_path='./embeddings/celeba_train_pca.pt',\n    celeba_source=source,\n    celeba_drive_paths=drive_paths\n)\n\nval_dataset_pca, val_loader_pca = get_dataset_and_loader(\n    dataset_name='celeba',\n    root=CONFIG['data_root'],\n    split='val',\n    image_size=CONFIG['image_size'],\n    batch_size=CONFIG['batch_size'],\n    num_workers=2,\n    pca_embeddings_path='./embeddings/celeba_val_pca.pt',\n    celeba_source=source,\n    celeba_drive_paths=drive_paths\n)\n\n# Optimizer\noptimizer_sdvae = torch.optim.AdamW(sd_vae.parameters(), lr=CONFIG['sd_vae_lr'])\n\n# Trainer\ntrainer_sdvae = MMRegTrainer(\n    vae=sd_vae,\n    loss_fn=loss_sdvae,\n    optimizer=optimizer_sdvae,\n    train_loader=train_loader_pca,\n    val_loader=val_loader_pca,\n    device=device,\n    save_dir='./checkpoints/celeba_sd_vae'\n)\n\n# Train\ntrainer_sdvae.train(num_epochs=CONFIG['sd_vae_epochs'])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## 4. Encode Dataset to SD Latents\n\nEncode all CelebA images to 4x16x16 SD latents (cached) and collect attributes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "from src.bottleneck_trainer import encode_dataset_to_sd_latents\nfrom torch.utils.data import DataLoader\n\nos.makedirs('./embeddings', exist_ok=True)\n\n# Create simple dataloaders for encoding (without PCA wrapper)\ntrain_loader_simple = DataLoader(\n    train_dataset_fixed,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=2\n)\nval_loader_simple = DataLoader(\n    val_dataset_fixed,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=2\n)\n\n# Encode all images to SD latents\nprint(\"Encoding training set to SD latents...\")\ntrain_sd_latents = encode_dataset_to_sd_latents(sd_vae, train_loader_simple, device)\ntorch.save(train_sd_latents, './embeddings/celeba_train_sd_latents.pt')\n\nprint(\"Encoding validation set to SD latents...\")\nval_sd_latents = encode_dataset_to_sd_latents(sd_vae, val_loader_simple, device)\ntorch.save(val_sd_latents, './embeddings/celeba_val_sd_latents.pt')\n\nprint(f\"\\nTrain SD latents: {train_sd_latents.shape}\")  # (N, 4, 16, 16)\nprint(f\"Val SD latents:   {val_sd_latents.shape}\")\n\n# Collect attributes in the same order\nprint(\"\\nCollecting attributes...\")\ntrain_attrs_list = []\nfor batch in tqdm(train_loader_simple, desc=\"Train attrs\"):\n    _, attrs = batch\n    train_attrs_list.append(attrs)\ntrain_attrs = torch.cat(train_attrs_list, dim=0)\n\nval_attrs_list = []\nfor batch in tqdm(val_loader_simple, desc=\"Val attrs\"):\n    _, attrs = batch\n    val_attrs_list.append(attrs)\nval_attrs = torch.cat(val_attrs_list, dim=0)\n\ntorch.save(train_attrs, './embeddings/celeba_train_attrs.pt')\ntorch.save(val_attrs, './embeddings/celeba_val_attrs.pt')\n\nprint(f\"Train attrs: {train_attrs.shape}\")  # (N, 40)\nprint(f\"Val attrs:   {val_attrs.shape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "## 5. Train Bottleneck VAEs\n\nTrain two bottleneck VAEs on the cached SD latents:\n- **Baseline**: reconstruction + KL only (lambda_mm = 0)\n- **MM-Reg**: reconstruction + KL + MM-Reg on 256-d bottleneck\n\n### 5.1 Baseline Bottleneck (no MM-Reg)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "from src.models.bottleneck import BottleneckVAE, BottleneckLoss\nfrom src.bottleneck_trainer import BottleneckTrainer\n\n# Load cached data\ntrain_sd_latents = torch.load('./embeddings/celeba_train_sd_latents.pt')\nval_sd_latents = torch.load('./embeddings/celeba_val_sd_latents.pt')\ntrain_pca = torch.load('./embeddings/celeba_train_pca.pt')\nval_pca = torch.load('./embeddings/celeba_val_pca.pt')\n\nprint(f\"Train SD latents: {train_sd_latents.shape}\")\nprint(f\"Train PCA: {train_pca.shape}\")\n\n# Spatial shape from actual latents\nspatial_shape = tuple(train_sd_latents.shape[1:])  # (4, 16, 16)\nprint(f\"Spatial shape: {spatial_shape}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING BASELINE BOTTLENECK (no MM-Reg)\")\nprint(\"=\"*60)\n\n# Baseline bottleneck\nbottleneck_baseline = BottleneckVAE(\n    spatial_shape=spatial_shape,\n    latent_dim=CONFIG['bottleneck_dim'],\n    hidden_dim=CONFIG['bottleneck_hidden']\n).to(device)\n\n# Loss without MM-Reg\nloss_baseline = BottleneckLoss(\n    lambda_mm=0.0,\n    beta=CONFIG['beta']\n)\n\noptimizer_baseline = torch.optim.Adam(bottleneck_baseline.parameters(), lr=CONFIG['bottleneck_lr'])\n\ntrainer_baseline = BottleneckTrainer(\n    bottleneck=bottleneck_baseline,\n    loss_fn=loss_baseline,\n    optimizer=optimizer_baseline,\n    train_latents=train_sd_latents,\n    train_pca=train_pca,\n    val_latents=val_sd_latents,\n    val_pca=val_pca,\n    batch_size=CONFIG['bottleneck_batch_size'],\n    device=device,\n    save_dir='./checkpoints/celeba_bottleneck_baseline'\n)\n\ntrainer_baseline.train(num_epochs=CONFIG['bottleneck_epochs'])"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "### 5.2 MM-Reg Bottleneck"
  },
  {
   "cell_type": "code",
   "id": "cell-16",
   "metadata": {},
   "source": "print(\"=\"*60)\nprint(\"TRAINING MM-REG BOTTLENECK\")\nprint(\"=\"*60)\n\n# MM-Reg bottleneck\nbottleneck_mmreg = BottleneckVAE(\n    spatial_shape=spatial_shape,\n    latent_dim=CONFIG['bottleneck_dim'],\n    hidden_dim=CONFIG['bottleneck_hidden']\n).to(device)\n\n# Loss with MM-Reg\nloss_mmreg = BottleneckLoss(\n    lambda_mm=CONFIG['lambda_mm'],\n    beta=CONFIG['beta'],\n    mm_variant='correlation'\n)\n\noptimizer_mmreg = torch.optim.Adam(bottleneck_mmreg.parameters(), lr=CONFIG['bottleneck_lr'])\n\ntrainer_mmreg = BottleneckTrainer(\n    bottleneck=bottleneck_mmreg,\n    loss_fn=loss_mmreg,\n    optimizer=optimizer_mmreg,\n    train_latents=train_sd_latents,\n    train_pca=train_pca,\n    val_latents=val_sd_latents,\n    val_pca=val_pca,\n    batch_size=CONFIG['bottleneck_batch_size'],\n    device=device,\n    save_dir='./checkpoints/celeba_bottleneck_mmreg'\n)\n\ntrainer_mmreg.train(num_epochs=CONFIG['bottleneck_epochs'])"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "## 6. Evaluate Bottleneck VAEs\n\nEvaluate end-to-end reconstruction (bottleneck decode → SD decode) and distance correlation in 256-d."
  },
  {
   "cell_type": "code",
   "id": "cell-18",
   "metadata": {},
   "source": "from src.models.losses import pairwise_distances, get_upper_triangular\nfrom scipy.stats import pearsonr, spearmanr\n\nval_sd_latents = torch.load('./embeddings/celeba_val_sd_latents.pt')\nval_pca = torch.load('./embeddings/celeba_val_pca.pt')\nval_attrs = torch.load('./embeddings/celeba_val_attrs.pt')\n\ndef evaluate_bottleneck(bottleneck, sd_vae, val_latents, val_pca, val_attrs, name, device):\n    \"\"\"Evaluate bottleneck: latent recon, end-to-end recon, distance correlation.\"\"\"\n    bottleneck.eval()\n    sd_vae.eval()\n    \n    all_z = []\n    total_latent_mse = 0\n    total_pixel_mse = 0\n    num_samples = 0\n    \n    with torch.no_grad():\n        for i in tqdm(range(0, len(val_latents), 64), desc=f\"Evaluating {name}\"):\n            batch_latents = val_latents[i:i+64].to(device)\n            \n            # Bottleneck forward\n            outputs = bottleneck(batch_latents, sample=False)\n            \n            # Latent-space reconstruction MSE\n            latent_mse = ((outputs['x_recon'] - batch_latents) ** 2).mean().item()\n            total_latent_mse += latent_mse * batch_latents.shape[0]\n            \n            # End-to-end pixel reconstruction: bottleneck recon → SD decode\n            pixel_recon = sd_vae.decode(outputs['x_recon'])\n            pixel_orig = sd_vae.decode(batch_latents)\n            pixel_mse = ((pixel_recon - pixel_orig) ** 2).mean().item()\n            total_pixel_mse += pixel_mse * batch_latents.shape[0]\n            \n            all_z.append(outputs['mu'].cpu())\n            num_samples += batch_latents.shape[0]\n    \n    all_z = torch.cat(all_z, dim=0)\n    \n    # Distance correlation in 256-d bottleneck space\n    n = min(500, len(all_z))\n    D_z = pairwise_distances(all_z[:n])\n    D_pca = pairwise_distances(val_pca[:n])\n    \n    d_z = get_upper_triangular(D_z).numpy()\n    d_pca = get_upper_triangular(D_pca).numpy()\n    \n    pearson, _ = pearsonr(d_z, d_pca)\n    spearman, _ = spearmanr(d_z, d_pca)\n    \n    results = {\n        'latent_mse': total_latent_mse / num_samples,\n        'pixel_mse': total_pixel_mse / num_samples,\n        'pearson_corr': pearson,\n        'spearman_corr': spearman,\n        'bottleneck_z': all_z,\n        'attrs': val_attrs\n    }\n    \n    print(f\"\\n{name} Results:\")\n    print(f\"  Latent Recon MSE:   {results['latent_mse']:.6f}\")\n    print(f\"  Pixel Recon MSE:    {results['pixel_mse']:.6f}\")\n    print(f\"  Pearson (256-d):    {results['pearson_corr']:.4f}\")\n    print(f\"  Spearman (256-d):   {results['spearman_corr']:.4f}\")\n    \n    return results\n\nresults_baseline = evaluate_bottleneck(\n    bottleneck_baseline, sd_vae, val_sd_latents, val_pca, val_attrs, \"Baseline Bottleneck\", device\n)\nresults_mmreg = evaluate_bottleneck(\n    bottleneck_mmreg, sd_vae, val_sd_latents, val_pca, val_attrs, \"MM-Reg Bottleneck\", device\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize end-to-end reconstructions: original → SD encode → bottleneck → SD decode\ndef plot_bottleneck_reconstructions(bottleneck, sd_vae, val_latents, val_loader_simple, title, save_path):\n    \"\"\"Show original images and their bottleneck reconstructions.\"\"\"\n    bottleneck.eval()\n    sd_vae.eval()\n    \n    # Get original images and their SD latents\n    batch = next(iter(val_loader_simple))\n    images = batch[0][:8].to(device)\n    \n    with torch.no_grad():\n        # Encode to SD latents\n        enc = sd_vae.encode(images, sample=False)\n        sd_latents = enc['latent']\n        \n        # Through bottleneck\n        bn_out = bottleneck(sd_latents, sample=False)\n        sd_recon = bn_out['x_recon']\n        \n        # Decode both through SD decoder\n        orig_decoded = sd_vae.decode(sd_latents)\n        bn_decoded = sd_vae.decode(sd_recon)\n    \n    fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n    fig.suptitle(title, fontsize=14)\n    \n    for i in range(8):\n        # Original\n        img = images[i].cpu().permute(1, 2, 0).numpy()\n        img = ((img + 1) / 2).clip(0, 1)\n        axes[0, i].imshow(img)\n        axes[0, i].axis('off')\n        \n        # SD VAE reconstruction (no bottleneck)\n        sd_rec = orig_decoded[i].cpu().permute(1, 2, 0).numpy()\n        sd_rec = ((sd_rec + 1) / 2).clip(0, 1)\n        axes[1, i].imshow(sd_rec)\n        axes[1, i].axis('off')\n        \n        # Bottleneck reconstruction (SD → bottleneck → SD)\n        bn_rec = bn_decoded[i].cpu().permute(1, 2, 0).numpy()\n        bn_rec = ((bn_rec + 1) / 2).clip(0, 1)\n        axes[2, i].imshow(bn_rec)\n        axes[2, i].axis('off')\n    \n    axes[0, 0].set_ylabel('Original', fontsize=11)\n    axes[1, 0].set_ylabel('SD VAE only', fontsize=11)\n    axes[2, 0].set_ylabel('+ Bottleneck', fontsize=11)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n\nplot_bottleneck_reconstructions(\n    bottleneck_baseline, sd_vae, val_sd_latents, val_loader_simple,\n    \"Baseline Bottleneck Reconstructions\",\n    \"./checkpoints/celeba_bottleneck_baseline/reconstructions.png\"\n)\nplot_bottleneck_reconstructions(\n    bottleneck_mmreg, sd_vae, val_sd_latents, val_loader_simple,\n    \"MM-Reg Bottleneck Reconstructions\",\n    \"./checkpoints/celeba_bottleneck_mmreg/reconstructions.png\"\n)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "## 7. Compute Attribute Directions in Bottleneck Space\n\nFor each attribute, compute: `d = mean(z_256d[attr=1]) - mean(z_256d[attr=0])`\n\nThese directions should work much better in the 256-d bottleneck than in the 4x16x16 spatial space."
  },
  {
   "cell_type": "code",
   "id": "cell-21",
   "metadata": {},
   "source": "# Encode training set through bottlenecks to get 256-d vectors + attrs\ntrain_attrs = torch.load('./embeddings/celeba_train_attrs.pt')\n\nprint(\"Encoding training set through baseline bottleneck...\")\ntrain_z_baseline = trainer_baseline.encode_all(train_sd_latents)\nprint(f\"Baseline z: {train_z_baseline.shape}\")  # (N, 256)\n\nprint(\"Encoding training set through MM-Reg bottleneck...\")\ntrain_z_mmreg = trainer_mmreg.encode_all(train_sd_latents)\nprint(f\"MM-Reg z: {train_z_mmreg.shape}\")\n\ndef compute_attribute_directions(z, attrs, attr_names):\n    \"\"\"Compute attribute directions in bottleneck space.\"\"\"\n    directions = {}\n    for attr_name in attr_names:\n        attr_idx = get_attr_idx(attr_name)\n        has_attr = attrs[:, attr_idx] > 0.5\n        no_attr = attrs[:, attr_idx] <= 0.5\n        \n        mean_with = z[has_attr].mean(dim=0)\n        mean_without = z[no_attr].mean(dim=0)\n        direction = mean_with - mean_without\n        directions[attr_name] = direction\n        \n        print(f\"  {attr_name}: {has_attr.sum().item()} with, {no_attr.sum().item()} without, ||d||={direction.norm():.2f}\")\n    return directions\n\nprint(\"\\nBaseline attribute directions:\")\ndirections_baseline = compute_attribute_directions(train_z_baseline, train_attrs, CONFIG['test_attributes'])\n\nprint(\"\\nMM-Reg attribute directions:\")\ndirections_mmreg = compute_attribute_directions(train_z_mmreg, train_attrs, CONFIG['test_attributes'])"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "## 8. Attribute Interpolation in Bottleneck Space\n\nManipulate attributes in the 256-d bottleneck space, then decode through:\n`z_256d + alpha*d → bottleneck decode → 4x16x16 → SD decode → image`"
  },
  {
   "cell_type": "code",
   "id": "cell-23",
   "metadata": {},
   "source": "def interpolate_bottleneck(bottleneck, sd_vae, sd_latent, direction, alphas, device):\n    \"\"\"\n    Interpolate attribute in bottleneck space, decode through full pipeline.\n    \n    Args:\n        bottleneck: BottleneckVAE\n        sd_vae: SD VAE (frozen decoder)\n        sd_latent: (1, 4, 16, 16) SD VAE latent\n        direction: (256,) attribute direction in bottleneck space\n        alphas: interpolation strengths\n        device: device\n    \n    Returns:\n        List of decoded images\n    \"\"\"\n    bottleneck.eval()\n    sd_vae.eval()\n    sd_latent = sd_latent.to(device)\n    direction = direction.to(device)\n    \n    with torch.no_grad():\n        # Encode to bottleneck\n        enc = bottleneck.encode(sd_latent, sample=False)\n        z = enc['mu']  # (1, 256)\n        \n        results = []\n        for alpha in alphas:\n            # Add direction in 256-d space\n            z_new = z + alpha * direction.unsqueeze(0)\n            \n            # Decode through bottleneck → 4x16x16\n            sd_recon = bottleneck.decode(z_new)\n            \n            # Decode through SD VAE → image\n            image = sd_vae.decode(sd_recon)\n            results.append(image.cpu())\n    \n    return results\n\n\ndef plot_bottleneck_interpolation(bottleneck, sd_vae, val_sd_latents, val_attrs,\n                                   directions, attr_name, title_prefix, save_path):\n    \"\"\"Plot attribute interpolation in bottleneck space.\"\"\"\n    attr_idx = get_attr_idx(attr_name)\n    \n    # Find samples without the attribute\n    no_attr_mask = val_attrs[:, attr_idx] <= 0.5\n    no_attr_indices = torch.where(no_attr_mask)[0][:4]\n    \n    if len(no_attr_indices) == 0:\n        print(f\"No test images without {attr_name}\")\n        return\n    \n    alphas = [-1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]\n    direction = directions[attr_name]\n    n_images = len(no_attr_indices)\n    \n    fig, axes = plt.subplots(n_images, len(alphas), figsize=(2*len(alphas), 2*n_images))\n    fig.suptitle(f\"{title_prefix}: {attr_name} Interpolation (bottleneck space)\", fontsize=14)\n    \n    for i, idx in enumerate(no_attr_indices):\n        sd_latent = val_sd_latents[idx:idx+1]\n        results = interpolate_bottleneck(bottleneck, sd_vae, sd_latent, direction, alphas, device)\n        \n        for j, (alpha, result) in enumerate(zip(alphas, results)):\n            img_np = result[0].permute(1, 2, 0).numpy()\n            img_np = ((img_np + 1) / 2).clip(0, 1)\n            \n            ax = axes[i, j] if n_images > 1 else axes[j]\n            ax.imshow(img_np)\n            ax.axis('off')\n            if i == 0:\n                ax.set_title(f\"a={alpha}\")\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# Test interpolation for each attribute\nos.makedirs('./checkpoints/interpolations', exist_ok=True)\n\nfor attr_name in CONFIG['test_attributes']:\n    print(f\"\\nTesting {attr_name} interpolation...\")\n    \n    plot_bottleneck_interpolation(\n        bottleneck_baseline, sd_vae, val_sd_latents, val_attrs,\n        directions_baseline, attr_name,\n        \"Baseline\", f\"./checkpoints/interpolations/baseline_{attr_name}.png\"\n    )\n    \n    plot_bottleneck_interpolation(\n        bottleneck_mmreg, sd_vae, val_sd_latents, val_attrs,\n        directions_mmreg, attr_name,\n        \"MM-Reg\", f\"./checkpoints/interpolations/mmreg_{attr_name}.png\"\n    )"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "## 9. Train Diffusion on 256-d Bottleneck Vectors\n\nThe diffusion model now learns to generate 256-d vectors (not 4x16x16).\nThis is a 1D diffusion problem - simpler and faster.\n\n### 9.1 Encode to Bottleneck Vectors"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "# Encode all SD latents to 256-d bottleneck vectors\nprint(\"Encoding to baseline bottleneck vectors...\")\ntrain_z_baseline = trainer_baseline.encode_all(train_sd_latents)\nval_z_baseline = trainer_baseline.encode_all(val_sd_latents)\nprint(f\"Baseline z - Train: {train_z_baseline.shape}, Val: {val_z_baseline.shape}\")\n\nprint(\"\\nEncoding to MM-Reg bottleneck vectors...\")\ntrain_z_mmreg = trainer_mmreg.encode_all(train_sd_latents)\nval_z_mmreg = trainer_mmreg.encode_all(val_sd_latents)\nprint(f\"MM-Reg z - Train: {train_z_mmreg.shape}, Val: {val_z_mmreg.shape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": "### 9.2 Train Diffusion on Baseline Bottleneck Vectors"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "from src.models.diffusion import MLPDenoiser, GaussianDiffusion\nfrom src.diffusion_trainer import DiffusionTrainer\n\nprint(\"=\"*60)\nprint(\"TRAINING DIFFUSION ON BASELINE BOTTLENECK (256-d)\")\nprint(\"=\"*60)\n\ndiffusion_baseline = GaussianDiffusion(\n    num_timesteps=CONFIG['diffusion_timesteps'],\n    device=device\n)\n\n# MLP denoiser for flat 256-d vectors\nmlp_baseline = MLPDenoiser(\n    input_dim=CONFIG['bottleneck_dim'],\n    hidden_dim=1024,\n    num_layers=6,\n    time_emb_dim=256\n).to(device)\n\nprint(f\"MLP params: {sum(p.numel() for p in mlp_baseline.parameters()):,}\")\n\noptimizer_diff_base = torch.optim.AdamW(mlp_baseline.parameters(), lr=CONFIG['diffusion_lr'])\n\ntrainer_diff_baseline = DiffusionTrainer(\n    model=mlp_baseline,\n    diffusion=diffusion_baseline,\n    optimizer=optimizer_diff_base,\n    train_latents=train_z_baseline,\n    val_latents=val_z_baseline,\n    batch_size=CONFIG['batch_size'],\n    device=device,\n    save_dir='./checkpoints/celeba_diffusion_baseline'\n)\n\ntrainer_diff_baseline.train(num_epochs=CONFIG['diffusion_epochs'])"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": "### 9.3 Train Diffusion on MM-Reg Bottleneck Vectors"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"TRAINING DIFFUSION ON MM-REG BOTTLENECK (256-d)\")\nprint(\"=\"*60)\n\ndiffusion_mmreg = GaussianDiffusion(\n    num_timesteps=CONFIG['diffusion_timesteps'],\n    device=device\n)\n\nmlp_mmreg = MLPDenoiser(\n    input_dim=CONFIG['bottleneck_dim'],\n    hidden_dim=1024,\n    num_layers=6,\n    time_emb_dim=256\n).to(device)\n\noptimizer_diff_mm = torch.optim.AdamW(mlp_mmreg.parameters(), lr=CONFIG['diffusion_lr'])\n\ntrainer_diff_mmreg = DiffusionTrainer(\n    model=mlp_mmreg,\n    diffusion=diffusion_mmreg,\n    optimizer=optimizer_diff_mm,\n    train_latents=train_z_mmreg,\n    val_latents=val_z_mmreg,\n    batch_size=CONFIG['batch_size'],\n    device=device,\n    save_dir='./checkpoints/celeba_diffusion_mmreg'\n)\n\ntrainer_diff_mmreg.train(num_epochs=CONFIG['diffusion_epochs'])"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": "## 10. Generate Samples & Apply Attribute Directions\n\nGenerate 256-d vectors with diffusion, then decode through:\n`z_256d → bottleneck decode → 4x16x16 → SD decode → image`"
  },
  {
   "cell_type": "code",
   "id": "cell-32",
   "metadata": {},
   "source": "# Generate 256-d samples from diffusion\nprint(\"Generating samples from Baseline Diffusion...\")\nsamples_z_baseline = trainer_diff_baseline.generate_samples(num_samples=16)\nprint(f\"Generated baseline z: {samples_z_baseline.shape}\")  # (16, 256)\n\nprint(\"\\nGenerating samples from MM-Reg Diffusion...\")\nsamples_z_mmreg = trainer_diff_mmreg.generate_samples(num_samples=16)\nprint(f\"Generated MM-Reg z: {samples_z_mmreg.shape}\")\n\n\ndef decode_bottleneck_to_images(bottleneck, sd_vae, z_vectors, device):\n    \"\"\"Decode 256-d vectors through bottleneck → SD VAE → images.\"\"\"\n    bottleneck.eval()\n    sd_vae.eval()\n    with torch.no_grad():\n        z = z_vectors.to(device)\n        # Bottleneck decode: 256-d → 4x16x16\n        sd_latents = bottleneck.decode(z)\n        # SD VAE decode: 4x16x16 → image\n        images = sd_vae.decode(sd_latents)\n    return images.cpu()\n\n\ndef plot_generated_faces(bottleneck, sd_vae, z_vectors, title, save_path):\n    \"\"\"Decode and plot generated faces.\"\"\"\n    images = decode_bottleneck_to_images(bottleneck, sd_vae, z_vectors, device)\n    \n    n = min(16, images.shape[0])\n    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n    fig.suptitle(title, fontsize=14)\n    \n    for i in range(n):\n        img = images[i].permute(1, 2, 0).numpy()\n        img = ((img + 1) / 2).clip(0, 1)\n        ax = axes[i // 8, i % 8]\n        ax.imshow(img)\n        ax.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n\nplot_generated_faces(\n    bottleneck_baseline, sd_vae, samples_z_baseline,\n    \"Generated Faces (Baseline Bottleneck + Diffusion)\",\n    \"./checkpoints/celeba_diffusion_baseline/generated_samples.png\"\n)\n\nplot_generated_faces(\n    bottleneck_mmreg, sd_vae, samples_z_mmreg,\n    \"Generated Faces (MM-Reg Bottleneck + Diffusion)\",\n    \"./checkpoints/celeba_diffusion_mmreg/generated_samples.png\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": "# Apply attribute directions to diffusion-generated 256-d vectors\ndef plot_generated_attr_interpolation(bottleneck, sd_vae, z_samples, directions,\n                                       attr_name, title_prefix, save_path):\n    \"\"\"Apply attribute direction to diffusion-generated bottleneck vectors.\"\"\"\n    bottleneck.eval()\n    sd_vae.eval()\n    \n    alphas = [-1.0, 0.0, 1.0, 2.0]\n    direction = directions[attr_name].to(device)\n    test_z = z_samples[:4].to(device)\n    \n    fig, axes = plt.subplots(4, len(alphas), figsize=(2*len(alphas), 8))\n    fig.suptitle(f\"{title_prefix}: {attr_name} on Generated Faces\", fontsize=14)\n    \n    with torch.no_grad():\n        for j, alpha in enumerate(alphas):\n            z_new = test_z + alpha * direction.unsqueeze(0)\n            \n            # Decode: 256-d → 4x16x16 → image\n            sd_latents = bottleneck.decode(z_new)\n            images = sd_vae.decode(sd_latents)\n            \n            for i in range(4):\n                img = images[i].cpu().permute(1, 2, 0).numpy()\n                img = ((img + 1) / 2).clip(0, 1)\n                axes[i, j].imshow(img)\n                axes[i, j].axis('off')\n                if i == 0:\n                    axes[i, j].set_title(f\"a={alpha}\")\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n\n\n# Test attribute manipulation on generated samples\nfor attr_name in CONFIG['test_attributes'][:3]:\n    print(f\"\\nApplying {attr_name} to generated samples...\")\n    \n    plot_generated_attr_interpolation(\n        bottleneck_baseline, sd_vae, samples_z_baseline, directions_baseline,\n        attr_name, \"Baseline\",\n        f\"./checkpoints/interpolations/gen_baseline_{attr_name}.png\"\n    )\n    \n    plot_generated_attr_interpolation(\n        bottleneck_mmreg, sd_vae, samples_z_mmreg, directions_mmreg,\n        attr_name, \"MM-Reg\",\n        f\"./checkpoints/interpolations/gen_mmreg_{attr_name}.png\"\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": "# Plot training curves\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bottleneck losses\nwith open('./checkpoints/celeba_bottleneck_baseline/history.json') as f:\n    baseline_bn_hist = json.load(f)\nwith open('./checkpoints/celeba_bottleneck_mmreg/history.json') as f:\n    mmreg_bn_hist = json.load(f)\n\naxes[0].plot([h['loss'] for h in baseline_bn_hist['train']], 'b-', label='Baseline Train')\naxes[0].plot([h['loss'] for h in baseline_bn_hist['val']], 'b--', label='Baseline Val')\naxes[0].plot([h['loss'] for h in mmreg_bn_hist['train']], 'r-', label='MM-Reg Train')\naxes[0].plot([h['loss'] for h in mmreg_bn_hist['val']], 'r--', label='MM-Reg Val')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Bottleneck VAE Training')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Diffusion losses\nwith open('./checkpoints/celeba_diffusion_baseline/history.json') as f:\n    baseline_diff_hist = json.load(f)\nwith open('./checkpoints/celeba_diffusion_mmreg/history.json') as f:\n    mmreg_diff_hist = json.load(f)\n\naxes[1].plot([h['loss'] for h in baseline_diff_hist['train']], 'b-', label='Baseline Train')\naxes[1].plot([h['loss'] for h in baseline_diff_hist['val']], 'b--', label='Baseline Val')\naxes[1].plot([h['loss'] for h in mmreg_diff_hist['train']], 'r-', label='MM-Reg Train')\naxes[1].plot([h['loss'] for h in mmreg_diff_hist['val']], 'r--', label='MM-Reg Val')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('Diffusion Training (on 256-d)')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('./checkpoints/celeba_training_comparison.png', dpi=150)\nplt.show()\n\n# Final summary\nprint(\"=\"*60)\nprint(\"CELEBA BOTTLENECK EXPERIMENT SUMMARY\")\nprint(\"=\"*60)\n\ndef to_python(val):\n    if hasattr(val, 'item'):\n        return val.item()\n    return float(val)\n\nsummary = {\n    'config': CONFIG,\n    'architecture': 'Image -> SD VAE -> 4x16x16 -> BottleneckVAE -> 256-d -> Diffusion',\n    'bottleneck_results': {\n        'baseline': {\n            'latent_mse': to_python(results_baseline['latent_mse']),\n            'pixel_mse': to_python(results_baseline['pixel_mse']),\n            'pearson_corr': to_python(results_baseline['pearson_corr']),\n            'spearman_corr': to_python(results_baseline['spearman_corr'])\n        },\n        'mmreg': {\n            'latent_mse': to_python(results_mmreg['latent_mse']),\n            'pixel_mse': to_python(results_mmreg['pixel_mse']),\n            'pearson_corr': to_python(results_mmreg['pearson_corr']),\n            'spearman_corr': to_python(results_mmreg['spearman_corr'])\n        }\n    },\n    'diffusion_final_loss': {\n        'baseline_train': to_python(baseline_diff_hist['train'][-1]['loss']),\n        'baseline_val': to_python(baseline_diff_hist['val'][-1]['loss']),\n        'mmreg_train': to_python(mmreg_diff_hist['train'][-1]['loss']),\n        'mmreg_val': to_python(mmreg_diff_hist['val'][-1]['loss'])\n    }\n}\n\nprint(\"\\nBottleneck VAE Comparison (256-d space):\")\nprint(f\"  Baseline - Latent MSE: {results_baseline['latent_mse']:.6f}, Pearson: {results_baseline['pearson_corr']:.4f}\")\nprint(f\"  MM-Reg   - Latent MSE: {results_mmreg['latent_mse']:.6f}, Pearson: {results_mmreg['pearson_corr']:.4f}\")\n\nprint(\"\\nDiffusion Final Val Loss (256-d):\")\nprint(f\"  Baseline: {summary['diffusion_final_loss']['baseline_val']:.6f}\")\nprint(f\"  MM-Reg:   {summary['diffusion_final_loss']['mmreg_val']:.6f}\")\n\nimprovement = (summary['diffusion_final_loss']['baseline_val'] - summary['diffusion_final_loss']['mmreg_val']) / summary['diffusion_final_loss']['baseline_val'] * 100\nprint(f\"\\nMM-Reg diffusion improvement: {improvement:.1f}%\")\n\nwith open('./checkpoints/celeba_experiment_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"\\nResults saved to ./checkpoints/celeba_experiment_summary.json\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}