{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# MM-Reg: CelebA Experiment with Attribute Interpolation\n",
    "\n",
    "End-to-end experiment on CelebA (200k faces, 40 attributes) comparing **Baseline VAE** vs **MM-Reg VAE**.\n",
    "\n",
    "## Pipeline:\n",
    "1. **Setup**: Install dependencies, load CelebA\n",
    "2. **Pre-compute PCA**: Reference embeddings for MM-Reg\n",
    "3. **Train VAEs**: Baseline and MM-Reg versions\n",
    "4. **Evaluate VAEs**: Reconstruction, distance correlation\n",
    "5. **Attribute Directions**: Find semantic directions in latent space\n",
    "6. **VAE Interpolation**: Test attribute manipulation on reconstructions\n",
    "7. **Train Diffusion**: On latents from both VAEs\n",
    "8. **Diffusion + Interpolation**: Apply attributes to generated samples\n",
    "\n",
    "**Hypothesis**: MM-Reg preserves manifold structure → semantic directions are more meaningful → smoother interpolations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf MMReg_diffusion_generative 2>/dev/null\n",
    "!git clone https://github.com/laurent-cheret/MMReg_diffusion_generative.git\n",
    "%cd MMReg_diffusion_generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch torchvision diffusers transformers accelerate\n!pip install -q pyyaml tqdm scipy scikit-learn matplotlib\n!pip install -q datasets  # For HuggingFace CelebA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_root': './data',\n",
    "    'image_size': 128,  # CelebA at 128x128\n",
    "    'batch_size': 64,\n",
    "    \n",
    "    # PCA\n",
    "    'pca_components': 256,\n",
    "    \n",
    "    # VAE Training\n",
    "    'vae_epochs': 5,\n",
    "    'vae_lr': 1e-5,\n",
    "    'lambda_mm': 1.0,\n",
    "    'beta': 1e-6,\n",
    "    \n",
    "    # Diffusion Training\n",
    "    'diffusion_epochs': 200,\n",
    "    'diffusion_lr': 1e-4,\n",
    "    'diffusion_timesteps': 1000,\n",
    "    \n",
    "    # Attributes to test for interpolation\n",
    "    'test_attributes': ['Smiling', 'Eyeglasses', 'Male', 'Young', 'Blond_Hair'],\n",
    "}\n",
    "\n",
    "# CelebA attribute names\n",
    "CELEBA_ATTRIBUTES = [\n",
    "    '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n",
    "    'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
    "    'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n",
    "    'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n",
    "    'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
    "    'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks',\n",
    "    'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
    "    'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'\n",
    "]\n",
    "\n",
    "def get_attr_idx(name):\n",
    "    \"\"\"Get index of attribute by name.\"\"\"\n",
    "    return CELEBA_ATTRIBUTES.index(name)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Load CelebA & Pre-compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "from src.data.dataset import (\n    get_celeba_dataset,\n    compute_pca_embeddings_celeba,\n    get_dataset_and_loader,\n    CELEBA_ATTRIBUTES\n)\n\n# === CONFIGURATION: Choose your data source ===\n# Option 1: HuggingFace (recommended, no rate limits)\nUSE_HUGGINGFACE = True\n\n# Option 2: Google Drive (if you have CelebA saved there)\n# Set these paths if you have CelebA in your Drive\nUSE_DRIVE = False\nDRIVE_PATHS = {\n    'images_zip': '/content/drive/MyDrive/DATASETS/CelebA/img_align_celeba.zip',\n    'attr_file': '/content/drive/MyDrive/DATASETS/CelebA/list_attr_celeba.txt',\n    'partition_file': '/content/drive/MyDrive/DATASETS/CelebA/list_eval_partition.txt'\n}\n\n# Mount Google Drive if using Drive source\nif USE_DRIVE:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n# Determine source\nif USE_DRIVE:\n    source = 'drive'\n    drive_paths = DRIVE_PATHS\nelse:\n    source = 'huggingface'\n    drive_paths = None\n\n# Load CelebA with fixed transforms for PCA\nprint(f\"Loading CelebA from {source}...\")\ntrain_dataset_fixed = get_celeba_dataset(\n    root=CONFIG['data_root'],\n    split='train',\n    image_size=CONFIG['image_size'],\n    fixed_transform=True,\n    source=source,\n    drive_paths=drive_paths\n)\n\nval_dataset_fixed = get_celeba_dataset(\n    root=CONFIG['data_root'],\n    split='val',\n    image_size=CONFIG['image_size'],\n    fixed_transform=True,\n    source=source,\n    drive_paths=drive_paths\n)\n\nprint(f\"Train: {len(train_dataset_fixed)}, Val: {len(val_dataset_fixed)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA embeddings\n",
    "os.makedirs('./embeddings', exist_ok=True)\n",
    "\n",
    "print(\"Computing PCA embeddings for training set...\")\n",
    "train_pca = compute_pca_embeddings_celeba(\n",
    "    train_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=64\n",
    ")\n",
    "torch.save(train_pca, './embeddings/celeba_train_pca.pt')\n",
    "\n",
    "print(\"\\nComputing PCA embeddings for validation set...\")\n",
    "val_pca = compute_pca_embeddings_celeba(\n",
    "    val_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=64\n",
    ")\n",
    "torch.save(val_pca, './embeddings/celeba_val_pca.pt')\n",
    "\n",
    "print(f\"\\nTrain PCA: {train_pca.shape}, Val PCA: {val_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Train VAEs\n",
    "\n",
    "### 3.1 Baseline VAE (No MM-Reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "from src.models.vae_wrapper import load_vae\nfrom src.models.losses import VAELoss\nfrom src.trainer import MMRegTrainer\n\nprint(\"=\"*60)\nprint(\"TRAINING BASELINE VAE (no MM-Reg)\")\nprint(\"=\"*60)\n\n# Load fresh VAE\nvae_baseline = load_vae(device=device)\n\n# Loss without MM-Reg\nloss_baseline = VAELoss(lambda_mm=0.0, beta=CONFIG['beta'])\n\n# Data loaders (use same source as earlier)\ntrain_dataset_base, train_loader_base = get_dataset_and_loader(\n    dataset_name='celeba',\n    root=CONFIG['data_root'],\n    split='train',\n    image_size=CONFIG['image_size'],\n    batch_size=CONFIG['batch_size'],\n    num_workers=2,\n    pca_embeddings_path='./embeddings/celeba_train_pca.pt',\n    celeba_source=source,\n    celeba_drive_paths=drive_paths\n)\n\nval_dataset_base, val_loader_base = get_dataset_and_loader(\n    dataset_name='celeba',\n    root=CONFIG['data_root'],\n    split='val',\n    image_size=CONFIG['image_size'],\n    batch_size=CONFIG['batch_size'],\n    num_workers=2,\n    pca_embeddings_path='./embeddings/celeba_val_pca.pt',\n    celeba_source=source,\n    celeba_drive_paths=drive_paths\n)\n\n# Optimizer\noptimizer_baseline = torch.optim.AdamW(vae_baseline.parameters(), lr=CONFIG['vae_lr'])\n\n# Trainer\ntrainer_baseline = MMRegTrainer(\n    vae=vae_baseline,\n    loss_fn=loss_baseline,\n    optimizer=optimizer_baseline,\n    train_loader=train_loader_base,\n    val_loader=val_loader_base,\n    device=device,\n    save_dir='./checkpoints/celeba_baseline_vae'\n)\n\n# Train\ntrainer_baseline.train(num_epochs=CONFIG['vae_epochs'])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 3.2 MM-Reg VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"TRAINING MM-REG VAE\")\nprint(\"=\"*60)\n\n# Load fresh VAE\nvae_mmreg = load_vae(device=device)\n\n# Loss with MM-Reg\nloss_mmreg = VAELoss(\n    lambda_mm=CONFIG['lambda_mm'],\n    beta=CONFIG['beta'],\n    mm_variant='correlation'\n)\n\n# Data loaders (use same source as earlier)\ntrain_dataset_mm, train_loader_mm = get_dataset_and_loader(\n    dataset_name='celeba',\n    root=CONFIG['data_root'],\n    split='train',\n    image_size=CONFIG['image_size'],\n    batch_size=CONFIG['batch_size'],\n    num_workers=2,\n    pca_embeddings_path='./embeddings/celeba_train_pca.pt',\n    celeba_source=source,\n    celeba_drive_paths=drive_paths\n)\n\nval_dataset_mm, val_loader_mm = get_dataset_and_loader(\n    dataset_name='celeba',\n    root=CONFIG['data_root'],\n    split='val',\n    image_size=CONFIG['image_size'],\n    batch_size=CONFIG['batch_size'],\n    num_workers=2,\n    pca_embeddings_path='./embeddings/celeba_val_pca.pt',\n    celeba_source=source,\n    celeba_drive_paths=drive_paths\n)\n\n# Optimizer\noptimizer_mmreg = torch.optim.AdamW(vae_mmreg.parameters(), lr=CONFIG['vae_lr'])\n\n# Trainer\ntrainer_mmreg = MMRegTrainer(\n    vae=vae_mmreg,\n    loss_fn=loss_mmreg,\n    optimizer=optimizer_mmreg,\n    train_loader=train_loader_mm,\n    val_loader=val_loader_mm,\n    device=device,\n    save_dir='./checkpoints/celeba_mmreg_vae'\n)\n\n# Train\ntrainer_mmreg.train(num_epochs=CONFIG['vae_epochs'])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Evaluate VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.losses import pairwise_distances, get_upper_triangular\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def evaluate_vae_celeba(vae, val_loader, name):\n",
    "    \"\"\"Evaluate VAE on CelebA: reconstruction + distance correlation.\"\"\"\n",
    "    vae.eval()\n",
    "    \n",
    "    all_latents = []\n",
    "    all_pca = []\n",
    "    all_attrs = []\n",
    "    total_recon_error = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Evaluating {name}\"):\n",
    "            images, attrs, pca_emb = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            outputs = vae(images, sample=False)\n",
    "            \n",
    "            recon_error = ((outputs['x_recon'] - images) ** 2).mean().item()\n",
    "            total_recon_error += recon_error * images.shape[0]\n",
    "            num_samples += images.shape[0]\n",
    "            \n",
    "            all_latents.append(outputs['latent_flat'].cpu())\n",
    "            all_pca.append(pca_emb)\n",
    "            all_attrs.append(attrs)\n",
    "    \n",
    "    all_latents = torch.cat(all_latents, dim=0)\n",
    "    all_pca = torch.cat(all_pca, dim=0)\n",
    "    all_attrs = torch.cat(all_attrs, dim=0)\n",
    "    \n",
    "    # Distance correlation (subset for speed)\n",
    "    n = min(500, len(all_latents))\n",
    "    D_latent = pairwise_distances(all_latents[:n])\n",
    "    D_pca = pairwise_distances(all_pca[:n])\n",
    "    \n",
    "    d_latent = get_upper_triangular(D_latent).numpy()\n",
    "    d_pca = get_upper_triangular(D_pca).numpy()\n",
    "    \n",
    "    pearson, _ = pearsonr(d_latent, d_pca)\n",
    "    spearman, _ = spearmanr(d_latent, d_pca)\n",
    "    \n",
    "    results = {\n",
    "        'recon_mse': total_recon_error / num_samples,\n",
    "        'pearson_corr': pearson,\n",
    "        'spearman_corr': spearman,\n",
    "        'latents': all_latents,\n",
    "        'attrs': all_attrs\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"  Reconstruction MSE: {results['recon_mse']:.6f}\")\n",
    "    print(f\"  Distance Pearson:   {results['pearson_corr']:.4f}\")\n",
    "    print(f\"  Distance Spearman:  {results['spearman_corr']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate both VAEs\n",
    "results_baseline = evaluate_vae_celeba(vae_baseline, val_loader_base, \"Baseline VAE\")\n",
    "results_mmreg = evaluate_vae_celeba(vae_mmreg, val_loader_mm, \"MM-Reg VAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "def plot_reconstructions_celeba(vae, val_loader, title, save_path):\n",
    "    vae.eval()\n",
    "    batch = next(iter(val_loader))\n",
    "    images = batch[0][:8].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = vae(images, sample=False)\n",
    "        recon = outputs['x_recon']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    \n",
    "    for i in range(8):\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = ((img + 1) / 2).clip(0, 1)\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        rec = recon[i].cpu().permute(1, 2, 0).numpy()\n",
    "        rec = ((rec + 1) / 2).clip(0, 1)\n",
    "        axes[1, i].imshow(rec)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Recon', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_reconstructions_celeba(vae_baseline, val_loader_base, \"Baseline VAE Reconstructions\", \n",
    "                            \"./checkpoints/celeba_baseline_vae/reconstructions.png\")\n",
    "plot_reconstructions_celeba(vae_mmreg, val_loader_mm, \"MM-Reg VAE Reconstructions\",\n",
    "                            \"./checkpoints/celeba_mmreg_vae/reconstructions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Compute Attribute Directions\n",
    "\n",
    "For each attribute, compute the direction in latent space:\n",
    "```\n",
    "d_attr = mean(z[attr=1]) - mean(z[attr=0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attribute_directions(latents, attrs, attr_names):\n",
    "    \"\"\"\n",
    "    Compute attribute directions in latent space.\n",
    "    \n",
    "    Args:\n",
    "        latents: (N, D) tensor of latent vectors\n",
    "        attrs: (N, 40) tensor of binary attributes\n",
    "        attr_names: List of attribute names to compute directions for\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping attribute name to direction vector\n",
    "    \"\"\"\n",
    "    directions = {}\n",
    "    \n",
    "    for attr_name in attr_names:\n",
    "        attr_idx = get_attr_idx(attr_name)\n",
    "        \n",
    "        # Get samples with and without attribute\n",
    "        has_attr = attrs[:, attr_idx] > 0.5\n",
    "        no_attr = attrs[:, attr_idx] <= 0.5\n",
    "        \n",
    "        # Compute means\n",
    "        mean_with = latents[has_attr].mean(dim=0)\n",
    "        mean_without = latents[no_attr].mean(dim=0)\n",
    "        \n",
    "        # Direction: adding this to a latent should add the attribute\n",
    "        direction = mean_with - mean_without\n",
    "        directions[attr_name] = direction\n",
    "        \n",
    "        print(f\"{attr_name}: {has_attr.sum().item()} with, {no_attr.sum().item()} without, ||d||={direction.norm():.2f}\")\n",
    "    \n",
    "    return directions\n",
    "\n",
    "print(\"Computing attribute directions for Baseline VAE...\")\n",
    "directions_baseline = compute_attribute_directions(\n",
    "    results_baseline['latents'],\n",
    "    results_baseline['attrs'],\n",
    "    CONFIG['test_attributes']\n",
    ")\n",
    "\n",
    "print(\"\\nComputing attribute directions for MM-Reg VAE...\")\n",
    "directions_mmreg = compute_attribute_directions(\n",
    "    results_mmreg['latents'],\n",
    "    results_mmreg['attrs'],\n",
    "    CONFIG['test_attributes']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6. VAE Attribute Interpolation\n",
    "\n",
    "Test if adding attribute direction to a latent produces the expected change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_attribute(vae, image, direction, alphas, device):\n",
    "    \"\"\"\n",
    "    Interpolate an attribute by adding scaled direction to latent.\n",
    "    \n",
    "    Args:\n",
    "        vae: VAE model\n",
    "        image: (1, C, H, W) input image\n",
    "        direction: (D,) attribute direction vector\n",
    "        alphas: List of interpolation strengths\n",
    "        device: Device\n",
    "    \n",
    "    Returns:\n",
    "        List of interpolated images\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    image = image.to(device)\n",
    "    direction = direction.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode\n",
    "        outputs = vae(image, sample=False)\n",
    "        z = outputs['latent_flat']  # (1, D)\n",
    "        \n",
    "        results = []\n",
    "        for alpha in alphas:\n",
    "            # Add scaled direction\n",
    "            z_new = z + alpha * direction.unsqueeze(0)\n",
    "            \n",
    "            # Reshape back to latent shape and decode\n",
    "            # For 128x128 images: latent is 16x16x4\n",
    "            latent_shape = outputs['latent'].shape\n",
    "            z_reshaped = z_new.view(latent_shape)\n",
    "            \n",
    "            # Decode\n",
    "            x_recon = vae.decode(z_reshaped)\n",
    "            results.append(x_recon.cpu())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_interpolation(vae, val_loader, directions, attr_name, title_prefix, save_path):\n",
    "    \"\"\"\n",
    "    Plot attribute interpolation for several test images.\n",
    "    \"\"\"\n",
    "    # Get test images that DON'T have the attribute\n",
    "    attr_idx = get_attr_idx(attr_name)\n",
    "    \n",
    "    batch = next(iter(val_loader))\n",
    "    images, attrs, _ = batch\n",
    "    \n",
    "    # Find images without the attribute\n",
    "    no_attr_mask = attrs[:, attr_idx] <= 0.5\n",
    "    test_images = images[no_attr_mask][:4]\n",
    "    \n",
    "    if len(test_images) == 0:\n",
    "        print(f\"No test images without {attr_name}\")\n",
    "        return\n",
    "    \n",
    "    alphas = [-1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]\n",
    "    direction = directions[attr_name]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(test_images), len(alphas), figsize=(2*len(alphas), 2*len(test_images)))\n",
    "    fig.suptitle(f\"{title_prefix}: {attr_name} Interpolation\", fontsize=14)\n",
    "    \n",
    "    for i, img in enumerate(test_images):\n",
    "        results = interpolate_attribute(vae, img.unsqueeze(0), direction, alphas, device)\n",
    "        \n",
    "        for j, (alpha, result) in enumerate(zip(alphas, results)):\n",
    "            img_np = result[0].permute(1, 2, 0).numpy()\n",
    "            img_np = ((img_np + 1) / 2).clip(0, 1)\n",
    "            \n",
    "            ax = axes[i, j] if len(test_images) > 1 else axes[j]\n",
    "            ax.imshow(img_np)\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title(f\"α={alpha}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test interpolation for each attribute\n",
    "os.makedirs('./checkpoints/interpolations', exist_ok=True)\n",
    "\n",
    "for attr_name in CONFIG['test_attributes']:\n",
    "    print(f\"\\nTesting {attr_name} interpolation...\")\n",
    "    \n",
    "    plot_interpolation(\n",
    "        vae_baseline, val_loader_base, directions_baseline, attr_name,\n",
    "        \"Baseline VAE\", f\"./checkpoints/interpolations/baseline_{attr_name}.png\"\n",
    "    )\n",
    "    \n",
    "    plot_interpolation(\n",
    "        vae_mmreg, val_loader_mm, directions_mmreg, attr_name,\n",
    "        \"MM-Reg VAE\", f\"./checkpoints/interpolations/mmreg_{attr_name}.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 7. Train Diffusion Models\n",
    "\n",
    "### 7.1 Encode Datasets to Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.diffusion_trainer import encode_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create simple dataloader for encoding (without PCA wrapper)\n",
    "train_loader_simple = DataLoader(\n",
    "    train_dataset_fixed,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader_simple = DataLoader(\n",
    "    val_dataset_fixed,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Encode with baseline VAE\n",
    "print(\"Encoding dataset with Baseline VAE...\")\n",
    "train_latents_baseline = encode_dataset(vae_baseline, train_loader_simple, device)\n",
    "val_latents_baseline = encode_dataset(vae_baseline, val_loader_simple, device)\n",
    "print(f\"Baseline latents - Train: {train_latents_baseline.shape}, Val: {val_latents_baseline.shape}\")\n",
    "\n",
    "# Encode with MM-Reg VAE\n",
    "print(\"\\nEncoding dataset with MM-Reg VAE...\")\n",
    "train_latents_mmreg = encode_dataset(vae_mmreg, train_loader_simple, device)\n",
    "val_latents_mmreg = encode_dataset(vae_mmreg, val_loader_simple, device)\n",
    "print(f\"MM-Reg latents - Train: {train_latents_mmreg.shape}, Val: {val_latents_mmreg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### 7.2 Train Diffusion on Baseline Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.diffusion import SimpleUNet, GaussianDiffusion\n",
    "from src.diffusion_trainer import DiffusionTrainer\n",
    "\n",
    "# Get latent shape for UNet configuration\n",
    "latent_size = train_latents_baseline.shape[2]  # Should be 16 for 128x128 images\n",
    "print(f\"Latent size: {latent_size}x{latent_size}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING DIFFUSION ON BASELINE LATENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diffusion_baseline = GaussianDiffusion(\n",
    "    num_timesteps=CONFIG['diffusion_timesteps'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Adjust UNet for smaller latent (16x16 instead of 32x32)\n",
    "# Use fewer resolution levels: (1, 2) gives 16->8 instead of (1,2,4) giving 32->16->8\n",
    "unet_baseline = SimpleUNet(\n",
    "    in_channels=4,\n",
    "    base_channels=128,\n",
    "    channel_mult=(1, 2, 4),  # Works for 16x16: 16->8->4\n",
    "    num_res_blocks=2\n",
    ").to(device)\n",
    "\n",
    "optimizer_diff_base = torch.optim.AdamW(unet_baseline.parameters(), lr=CONFIG['diffusion_lr'])\n",
    "\n",
    "trainer_diff_baseline = DiffusionTrainer(\n",
    "    model=unet_baseline,\n",
    "    diffusion=diffusion_baseline,\n",
    "    optimizer=optimizer_diff_base,\n",
    "    train_latents=train_latents_baseline,\n",
    "    val_latents=val_latents_baseline,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/celeba_diffusion_baseline'\n",
    ")\n",
    "\n",
    "trainer_diff_baseline.train(num_epochs=CONFIG['diffusion_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### 7.3 Train Diffusion on MM-Reg Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING DIFFUSION ON MM-REG LATENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diffusion_mmreg = GaussianDiffusion(\n",
    "    num_timesteps=CONFIG['diffusion_timesteps'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "unet_mmreg = SimpleUNet(\n",
    "    in_channels=4,\n",
    "    base_channels=128,\n",
    "    channel_mult=(1, 2, 4),\n",
    "    num_res_blocks=2\n",
    ").to(device)\n",
    "\n",
    "optimizer_diff_mm = torch.optim.AdamW(unet_mmreg.parameters(), lr=CONFIG['diffusion_lr'])\n",
    "\n",
    "trainer_diff_mmreg = DiffusionTrainer(\n",
    "    model=unet_mmreg,\n",
    "    diffusion=diffusion_mmreg,\n",
    "    optimizer=optimizer_diff_mm,\n",
    "    train_latents=train_latents_mmreg,\n",
    "    val_latents=val_latents_mmreg,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/celeba_diffusion_mmreg'\n",
    ")\n",
    "\n",
    "trainer_diff_mmreg.train(num_epochs=CONFIG['diffusion_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 8. Generate Samples & Apply Attribute Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate latent samples from both diffusion models\n",
    "print(\"Generating samples from Baseline Diffusion...\")\n",
    "# Shape for 128x128 images: (N, 4, 16, 16)\n",
    "latent_shape = (16, 4, latent_size, latent_size)\n",
    "samples_baseline = trainer_diff_baseline.generate_samples(num_samples=16)\n",
    "\n",
    "print(\"\\nGenerating samples from MM-Reg Diffusion...\")\n",
    "samples_mmreg = trainer_diff_mmreg.generate_samples(num_samples=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and visualize generated samples\n",
    "def decode_and_plot_celeba(vae, latents, title, save_path):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        latents = latents.to(device)\n",
    "        images = vae.decode(latents)\n",
    "    \n",
    "    n = min(16, images.shape[0])\n",
    "    rows = 2\n",
    "    cols = 8\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 4))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    \n",
    "    for i in range(n):\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = ((img + 1) / 2).clip(0, 1)\n",
    "        ax = axes[i // cols, i % cols]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "decode_and_plot_celeba(vae_baseline, samples_baseline, \n",
    "                       \"Generated Faces (Baseline VAE + Diffusion)\",\n",
    "                       \"./checkpoints/celeba_diffusion_baseline/generated_samples.png\")\n",
    "\n",
    "decode_and_plot_celeba(vae_mmreg, samples_mmreg,\n",
    "                       \"Generated Faces (MM-Reg VAE + Diffusion)\",\n",
    "                       \"./checkpoints/celeba_diffusion_mmreg/generated_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply attribute directions to diffusion-generated samples\n",
    "def apply_direction_to_generated(vae, latent_samples, direction, alphas, device):\n",
    "    \"\"\"\n",
    "    Apply attribute direction to diffusion-generated latents.\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    latent_samples = latent_samples.to(device)\n",
    "    direction = direction.to(device)\n",
    "    \n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for alpha in alphas:\n",
    "            # Flatten latent, add direction, reshape back\n",
    "            z_flat = latent_samples.view(latent_samples.shape[0], -1)\n",
    "            z_new = z_flat + alpha * direction.unsqueeze(0)\n",
    "            z_reshaped = z_new.view(latent_samples.shape)\n",
    "            \n",
    "            # Decode\n",
    "            images = vae.decode(z_reshaped)\n",
    "            results.append(images.cpu())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_generated_interpolation(vae, latent_samples, directions, attr_name, title_prefix, save_path):\n",
    "    \"\"\"\n",
    "    Plot attribute interpolation on diffusion-generated samples.\n",
    "    \"\"\"\n",
    "    alphas = [-1.0, 0.0, 1.0, 2.0]\n",
    "    direction = directions[attr_name]\n",
    "    \n",
    "    # Use first 4 samples\n",
    "    test_latents = latent_samples[:4]\n",
    "    \n",
    "    results = apply_direction_to_generated(vae, test_latents, direction, alphas, device)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, len(alphas), figsize=(2*len(alphas), 8))\n",
    "    fig.suptitle(f\"{title_prefix}: {attr_name} on Generated Samples\", fontsize=14)\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j, (alpha, imgs) in enumerate(zip(alphas, results)):\n",
    "            img_np = imgs[i].permute(1, 2, 0).numpy()\n",
    "            img_np = ((img_np + 1) / 2).clip(0, 1)\n",
    "            \n",
    "            axes[i, j].imshow(img_np)\n",
    "            axes[i, j].axis('off')\n",
    "            if i == 0:\n",
    "                axes[i, j].set_title(f\"α={alpha}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attribute manipulation on generated samples\n",
    "for attr_name in CONFIG['test_attributes'][:3]:  # Test first 3 attributes\n",
    "    print(f\"\\nApplying {attr_name} to generated samples...\")\n",
    "    \n",
    "    plot_generated_interpolation(\n",
    "        vae_baseline, samples_baseline, directions_baseline, attr_name,\n",
    "        \"Baseline\", f\"./checkpoints/interpolations/gen_baseline_{attr_name}.png\"\n",
    "    )\n",
    "    \n",
    "    plot_generated_interpolation(\n",
    "        vae_mmreg, samples_mmreg, directions_mmreg, attr_name,\n",
    "        \"MM-Reg\", f\"./checkpoints/interpolations/gen_mmreg_{attr_name}.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 9. Training Curves & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# VAE losses\n",
    "with open('./checkpoints/celeba_baseline_vae/history.json') as f:\n",
    "    baseline_vae_hist = json.load(f)\n",
    "with open('./checkpoints/celeba_mmreg_vae/history.json') as f:\n",
    "    mmreg_vae_hist = json.load(f)\n",
    "\n",
    "axes[0].plot([h['loss'] for h in baseline_vae_hist['train']], 'b-', label='Baseline Train')\n",
    "axes[0].plot([h['loss'] for h in baseline_vae_hist['val']], 'b--', label='Baseline Val')\n",
    "axes[0].plot([h['loss'] for h in mmreg_vae_hist['train']], 'r-', label='MM-Reg Train')\n",
    "axes[0].plot([h['loss'] for h in mmreg_vae_hist['val']], 'r--', label='MM-Reg Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('VAE Training')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Diffusion losses\n",
    "with open('./checkpoints/celeba_diffusion_baseline/history.json') as f:\n",
    "    baseline_diff_hist = json.load(f)\n",
    "with open('./checkpoints/celeba_diffusion_mmreg/history.json') as f:\n",
    "    mmreg_diff_hist = json.load(f)\n",
    "\n",
    "axes[1].plot([h['loss'] for h in baseline_diff_hist['train']], 'b-', label='Baseline Train')\n",
    "axes[1].plot([h['loss'] for h in baseline_diff_hist['val']], 'b--', label='Baseline Val')\n",
    "axes[1].plot([h['loss'] for h in mmreg_diff_hist['train']], 'r-', label='MM-Reg Train')\n",
    "axes[1].plot([h['loss'] for h in mmreg_diff_hist['val']], 'r--', label='MM-Reg Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Diffusion Training')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./checkpoints/celeba_training_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"CELEBA EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def to_python(val):\n",
    "    if hasattr(val, 'item'):\n",
    "        return val.item()\n",
    "    return float(val)\n",
    "\n",
    "summary = {\n",
    "    'config': CONFIG,\n",
    "    'vae_results': {\n",
    "        'baseline': {\n",
    "            'recon_mse': to_python(results_baseline['recon_mse']),\n",
    "            'pearson_corr': to_python(results_baseline['pearson_corr']),\n",
    "            'spearman_corr': to_python(results_baseline['spearman_corr'])\n",
    "        },\n",
    "        'mmreg': {\n",
    "            'recon_mse': to_python(results_mmreg['recon_mse']),\n",
    "            'pearson_corr': to_python(results_mmreg['pearson_corr']),\n",
    "            'spearman_corr': to_python(results_mmreg['spearman_corr'])\n",
    "        }\n",
    "    },\n",
    "    'diffusion_final_loss': {\n",
    "        'baseline_train': to_python(baseline_diff_hist['train'][-1]['loss']),\n",
    "        'baseline_val': to_python(baseline_diff_hist['val'][-1]['loss']),\n",
    "        'mmreg_train': to_python(mmreg_diff_hist['train'][-1]['loss']),\n",
    "        'mmreg_val': to_python(mmreg_diff_hist['val'][-1]['loss'])\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nVAE Comparison:\")\n",
    "print(f\"  Baseline - Recon MSE: {results_baseline['recon_mse']:.6f}, Pearson: {results_baseline['pearson_corr']:.4f}\")\n",
    "print(f\"  MM-Reg   - Recon MSE: {results_mmreg['recon_mse']:.6f}, Pearson: {results_mmreg['pearson_corr']:.4f}\")\n",
    "\n",
    "print(\"\\nDiffusion Final Val Loss:\")\n",
    "print(f\"  Baseline: {summary['diffusion_final_loss']['baseline_val']:.6f}\")\n",
    "print(f\"  MM-Reg:   {summary['diffusion_final_loss']['mmreg_val']:.6f}\")\n",
    "\n",
    "improvement = (summary['diffusion_final_loss']['baseline_val'] - summary['diffusion_final_loss']['mmreg_val']) / summary['diffusion_final_loss']['baseline_val'] * 100\n",
    "print(f\"\\nMM-Reg diffusion improvement: {improvement:.1f}%\")\n",
    "\n",
    "# Save summary\n",
    "with open('./checkpoints/celeba_experiment_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to ./checkpoints/celeba_experiment_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}