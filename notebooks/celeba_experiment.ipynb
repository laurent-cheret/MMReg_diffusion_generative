{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# MM-Reg: CelebA Experiment (SD VAE + Diffusion)\n\nDoes MM-Reg regularization improve diffusion generation quality on CelebA faces?\n\n## Design:\n1. Fine-tune TWO SD VAEs on full CelebA (~160k): **Baseline** vs **MM-Reg**\n2. Encode all images with both VAEs (cached as 4x16x16 latents)\n3. Train diffusion models on each set of latents\n4. Compare diffusion val loss, generate samples, and evaluate\n\n**Key**: Same architecture, same data, only difference is MM-Reg during VAE training."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf MMReg_diffusion_generative 2>/dev/null\n",
    "!git clone https://github.com/laurent-cheret/MMReg_diffusion_generative.git\n",
    "%cd MMReg_diffusion_generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision diffusers transformers accelerate\n",
    "!pip install -q pyyaml tqdm scipy scikit-learn matplotlib\n",
    "!pip install -q datasets gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Experiment configuration\nCONFIG = {\n    # Data\n    'data_root': './data',\n    'image_size': 128,\n    'batch_size': 64,\n\n    # PCA\n    'pca_components': 256,\n\n    # VAE Training (on full dataset)\n    'vae_epochs': 5,\n    'vae_lr': 1e-5,\n    'lambda_mm': 1.0,\n    'beta': 1e-6,\n\n    # Diffusion Training\n    'diffusion_epochs': 50,\n    'diffusion_lr': 1e-4,\n    'diffusion_timesteps': 1000,\n}\n\nprint(\"Configuration:\")\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Load CelebA & Pre-compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "from src.data.dataset import (\n    get_celeba_dataset,\n    compute_pca_embeddings_celeba,\n    get_dataset_and_loader\n)\n\n# === CONFIGURATION: Choose your data source ===\n# Option 1: HuggingFace (recommended)\nUSE_HUGGINGFACE = True\n\n# Option 2: Google Drive\nUSE_DRIVE = False\nDRIVE_PATHS = {\n    'images_zip': '/content/drive/MyDrive/DATASETS/CelebA/img_align_celeba.zip',\n    'attr_file': '/content/drive/MyDrive/DATASETS/CelebA/list_attr_celeba.txt',\n    'partition_file': '/content/drive/MyDrive/DATASETS/CelebA/list_eval_partition.txt'\n}\n\nif USE_DRIVE:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\nsource = 'drive' if USE_DRIVE else 'huggingface'\ndrive_paths = DRIVE_PATHS if USE_DRIVE else None\n\n# Load full CelebA with fixed transforms for PCA\nprint(f\"Loading CelebA from {source}...\")\ntrain_dataset_fixed = get_celeba_dataset(\n    root=CONFIG['data_root'],\n    split='train',\n    image_size=CONFIG['image_size'],\n    fixed_transform=True,\n    source=source,\n    drive_paths=drive_paths\n)\n\nval_dataset_fixed = get_celeba_dataset(\n    root=CONFIG['data_root'],\n    split='val',\n    image_size=CONFIG['image_size'],\n    fixed_transform=True,\n    source=source,\n    drive_paths=drive_paths\n)\n\ntotal_train = len(train_dataset_fixed)\nprint(f\"Train: {total_train}, Val: {len(val_dataset_fixed)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA embeddings on full training set\n",
    "os.makedirs('./embeddings', exist_ok=True)\n",
    "\n",
    "print(\"Computing PCA embeddings for training set...\")\n",
    "train_pca = compute_pca_embeddings_celeba(\n",
    "    train_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=256\n",
    ")\n",
    "torch.save(train_pca, './embeddings/celeba_train_pca.pt')\n",
    "\n",
    "print(\"Computing PCA embeddings for validation set...\")\n",
    "val_pca = compute_pca_embeddings_celeba(\n",
    "    val_dataset_fixed,\n",
    "    n_components=CONFIG['pca_components'],\n",
    "    batch_size=256\n",
    ")\n",
    "torch.save(val_pca, './embeddings/celeba_val_pca.pt')\n",
    "\n",
    "print(f\"Train PCA: {train_pca.shape}, Val PCA: {val_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## 3. Fine-tune VAEs on Full CelebA\n\nTrain two SD VAEs on the full dataset:\n- **Baseline**: reconstruction + KL only (no MM-Reg)\n- **MM-Reg**: reconstruction + KL + MM-Reg regularization\n\n### 3.1 Baseline VAE (no MM-Reg)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.vae_wrapper import load_vae\n",
    "from src.models.losses import VAELoss\n",
    "from src.trainer import MMRegTrainer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING BASELINE VAE (no MM-Reg) on full CelebA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vae_baseline = load_vae(device=device)\n",
    "\n",
    "loss_baseline = VAELoss(lambda_mm=0.0, beta=CONFIG['beta'])\n",
    "\n",
    "train_dataset_base, train_loader_base = get_dataset_and_loader(\n",
    "    dataset_name='celeba',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='train',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/celeba_train_pca.pt',\n",
    "    celeba_source=source,\n",
    "    celeba_drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "val_dataset_base, val_loader_base = get_dataset_and_loader(\n",
    "    dataset_name='celeba',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='val',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/celeba_val_pca.pt',\n",
    "    celeba_source=source,\n",
    "    celeba_drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "optimizer_baseline = torch.optim.AdamW(vae_baseline.parameters(), lr=CONFIG['vae_lr'])\n",
    "\n",
    "trainer_baseline = MMRegTrainer(\n",
    "    vae=vae_baseline,\n",
    "    loss_fn=loss_baseline,\n",
    "    optimizer=optimizer_baseline,\n",
    "    train_loader=train_loader_base,\n",
    "    val_loader=val_loader_base,\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/scaling_baseline_vae'\n",
    ")\n",
    "\n",
    "trainer_baseline.train(num_epochs=CONFIG['vae_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 3.2 MM-Reg VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING MM-REG VAE on full CelebA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vae_mmreg = load_vae(device=device)\n",
    "\n",
    "loss_mmreg = VAELoss(\n",
    "    lambda_mm=CONFIG['lambda_mm'],\n",
    "    beta=CONFIG['beta'],\n",
    "    mm_variant='correlation'\n",
    ")\n",
    "\n",
    "train_dataset_mm, train_loader_mm = get_dataset_and_loader(\n",
    "    dataset_name='celeba',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='train',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/celeba_train_pca.pt',\n",
    "    celeba_source=source,\n",
    "    celeba_drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "val_dataset_mm, val_loader_mm = get_dataset_and_loader(\n",
    "    dataset_name='celeba',\n",
    "    root=CONFIG['data_root'],\n",
    "    split='val',\n",
    "    image_size=CONFIG['image_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,\n",
    "    pca_embeddings_path='./embeddings/celeba_val_pca.pt',\n",
    "    celeba_source=source,\n",
    "    celeba_drive_paths=drive_paths\n",
    ")\n",
    "\n",
    "optimizer_mmreg = torch.optim.AdamW(vae_mmreg.parameters(), lr=CONFIG['vae_lr'])\n",
    "\n",
    "trainer_mmreg = MMRegTrainer(\n",
    "    vae=vae_mmreg,\n",
    "    loss_fn=loss_mmreg,\n",
    "    optimizer=optimizer_mmreg,\n",
    "    train_loader=train_loader_mm,\n",
    "    val_loader=val_loader_mm,\n",
    "    device=device,\n",
    "    save_dir='./checkpoints/scaling_mmreg_vae'\n",
    ")\n",
    "\n",
    "trainer_mmreg.train(num_epochs=CONFIG['vae_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Evaluate VAEs & Encode Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.losses import pairwise_distances, get_upper_triangular\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def quick_evaluate(vae, val_loader, val_pca, name):\n",
    "    \"\"\"Quick VAE evaluation: recon MSE + Pearson correlation.\"\"\"\n",
    "    vae.eval()\n",
    "    all_latents = []\n",
    "    total_recon = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Eval {name}\"):\n",
    "            images = batch[0].to(device)\n",
    "            outputs = vae(images, sample=False)\n",
    "            total_recon += ((outputs['x_recon'] - images) ** 2).mean().item() * images.shape[0]\n",
    "            num_samples += images.shape[0]\n",
    "            all_latents.append(outputs['latent_flat'].cpu())\n",
    "\n",
    "    all_latents = torch.cat(all_latents, dim=0)\n",
    "\n",
    "    n = min(500, len(all_latents))\n",
    "    D_lat = pairwise_distances(all_latents[:n])\n",
    "    D_pca = pairwise_distances(val_pca[:n])\n",
    "    d_lat = get_upper_triangular(D_lat).numpy()\n",
    "    d_pca = get_upper_triangular(D_pca).numpy()\n",
    "    pearson, _ = pearsonr(d_lat, d_pca)\n",
    "\n",
    "    recon_mse = total_recon / num_samples\n",
    "    print(f\"{name}: Recon MSE={recon_mse:.6f}, Pearson={pearson:.4f}\")\n",
    "    return {'recon_mse': recon_mse, 'pearson': pearson}\n",
    "\n",
    "print(\"\\nVAE Evaluation (on full val set):\")\n",
    "vae_results_baseline = quick_evaluate(vae_baseline, val_loader_base, val_pca, \"Baseline\")\n",
    "vae_results_mmreg = quick_evaluate(vae_mmreg, val_loader_mm, val_pca, \"MM-Reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "from src.diffusion_trainer import encode_dataset\n\n# Simple dataloaders for encoding (no PCA wrapper)\ntrain_loader_simple = DataLoader(\n    train_dataset_fixed,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=2\n)\nval_loader_simple = DataLoader(\n    val_dataset_fixed,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=2\n)\n\n# Encode full dataset with both VAEs\nprint(\"Encoding full dataset with Baseline VAE...\")\ntrain_latents_baseline = encode_dataset(vae_baseline, train_loader_simple, device)\nval_latents_baseline = encode_dataset(vae_baseline, val_loader_simple, device)\nprint(f\"Baseline latents - Train: {train_latents_baseline.shape}, Val: {val_latents_baseline.shape}\")\n\nprint(\"\\nEncoding full dataset with MM-Reg VAE...\")\ntrain_latents_mmreg = encode_dataset(vae_mmreg, train_loader_simple, device)\nval_latents_mmreg = encode_dataset(vae_mmreg, val_loader_simple, device)\nprint(f\"MM-Reg latents - Train: {train_latents_mmreg.shape}, Val: {val_latents_mmreg.shape}\")\n\n# Save for potential reuse\ntorch.save(train_latents_baseline, './embeddings/celeba_train_latents_baseline.pt')\ntorch.save(val_latents_baseline, './embeddings/celeba_val_latents_baseline.pt')\ntorch.save(train_latents_mmreg, './embeddings/celeba_train_latents_mmreg.pt')\ntorch.save(val_latents_mmreg, './embeddings/celeba_val_latents_mmreg.pt')\nprint(\"\\nLatents cached to ./embeddings/\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "## 5. Train Diffusion Models\n\nTrain two diffusion models on the full set of cached latents:\n- **Baseline**: trained on latents from baseline VAE\n- **MM-Reg**: trained on latents from MM-Reg VAE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "from src.models.diffusion import DiT, GaussianDiffusion\nfrom src.diffusion_trainer import DiffusionTrainer\n\nlatent_size = train_latents_baseline.shape[2]  # 16 for 128x128 images\nprint(f\"Latent size: {latent_size}x{latent_size}\")\nprint(f\"Training samples: {len(train_latents_baseline):,}\")\n\n# --- Baseline Diffusion (DiT-S) ---\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING BASELINE DIFFUSION (DiT-S)\")\nprint(f\"{'='*60}\")\n\ndiffusion_base = GaussianDiffusion(\n    num_timesteps=CONFIG['diffusion_timesteps'],\n    device=device\n)\n\ndit_base = DiT(\n    in_channels=4,\n    patch_size=2,\n    hidden_size=384,\n    depth=12,\n    num_heads=6,\n    mlp_ratio=4.0,\n    input_size=latent_size,\n).to(device)\n\nprint(f\"DiT-S params: {sum(p.numel() for p in dit_base.parameters()):,}\")\n\nopt_base = torch.optim.AdamW(dit_base.parameters(), lr=CONFIG['diffusion_lr'])\n\ntrainer_diff_baseline = DiffusionTrainer(\n    model=dit_base,\n    diffusion=diffusion_base,\n    optimizer=opt_base,\n    train_latents=train_latents_baseline,\n    val_latents=val_latents_baseline,\n    batch_size=CONFIG['batch_size'],\n    device=device,\n    save_dir='./checkpoints/celeba_diff_baseline'\n)\n\ntrainer_diff_baseline.train(num_epochs=CONFIG['diffusion_epochs'])\n\n# --- MM-Reg Diffusion (DiT-S) ---\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING MM-REG DIFFUSION (DiT-S)\")\nprint(f\"{'='*60}\")\n\ndiffusion_mm = GaussianDiffusion(\n    num_timesteps=CONFIG['diffusion_timesteps'],\n    device=device\n)\n\ndit_mm = DiT(\n    in_channels=4,\n    patch_size=2,\n    hidden_size=384,\n    depth=12,\n    num_heads=6,\n    mlp_ratio=4.0,\n    input_size=latent_size,\n).to(device)\n\nopt_mm = torch.optim.AdamW(dit_mm.parameters(), lr=CONFIG['diffusion_lr'])\n\ntrainer_diff_mmreg = DiffusionTrainer(\n    model=dit_mm,\n    diffusion=diffusion_mm,\n    optimizer=opt_mm,\n    train_latents=train_latents_mmreg,\n    val_latents=val_latents_mmreg,\n    batch_size=CONFIG['batch_size'],\n    device=device,\n    save_dir='./checkpoints/celeba_diff_mmreg'\n)\n\ntrainer_diff_mmreg.train(num_epochs=CONFIG['diffusion_epochs'])\n\n# Record results\nbase_val = trainer_diff_baseline.val_history[-1]['loss']\nmm_val = trainer_diff_mmreg.val_history[-1]['loss']\nimprovement = (base_val - mm_val) / base_val * 100\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Baseline val loss: {base_val:.6f}\")\nprint(f\"MM-Reg val loss:   {mm_val:.6f}\")\nprint(f\"Improvement:       {improvement:.1f}%\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "## 6. Results & Visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Training curves comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# VAE training curves\nwith open('./checkpoints/scaling_baseline_vae/history.json') as f:\n    base_vae_hist = json.load(f)\nwith open('./checkpoints/scaling_mmreg_vae/history.json') as f:\n    mm_vae_hist = json.load(f)\n\naxes[0].plot([h['loss'] for h in base_vae_hist['train']], 'b-', label='Baseline Train')\naxes[0].plot([h['loss'] for h in base_vae_hist['val']], 'b--', label='Baseline Val')\naxes[0].plot([h['loss'] for h in mm_vae_hist['train']], 'r-', label='MM-Reg Train')\naxes[0].plot([h['loss'] for h in mm_vae_hist['val']], 'r--', label='MM-Reg Val')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('VAE Training')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Diffusion training curves\nbase_diff_curve = [h['loss'] for h in trainer_diff_baseline.val_history]\nmm_diff_curve = [h['loss'] for h in trainer_diff_mmreg.val_history]\n\naxes[1].plot(base_diff_curve, 'b-', label='Baseline Val')\naxes[1].plot(mm_diff_curve, 'r-', label='MM-Reg Val')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('Diffusion Training (Val Loss)')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('./checkpoints/celeba_training_curves.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Generate sample faces for visual comparison\nprint(\"Loading best diffusion models...\")\n\ndiffusion_gen = GaussianDiffusion(\n    num_timesteps=CONFIG['diffusion_timesteps'],\n    device=device\n)\n\n# Baseline DiT-S\ndit_gen_base = DiT(in_channels=4, patch_size=2, hidden_size=384, depth=12, num_heads=6, input_size=latent_size).to(device)\nckpt_base = torch.load('./checkpoints/celeba_diff_baseline/best.pt', map_location=device)\ndit_gen_base.load_state_dict(ckpt_base['model_state_dict'])\n\n# MM-Reg DiT-S\ndit_gen_mm = DiT(in_channels=4, patch_size=2, hidden_size=384, depth=12, num_heads=6, input_size=latent_size).to(device)\nckpt_mm = torch.load('./checkpoints/celeba_diff_mmreg/best.pt', map_location=device)\ndit_gen_mm.load_state_dict(ckpt_mm['model_state_dict'])\n\n# Generate\nprint(\"Generating baseline samples...\")\nlatent_shape = (16, 4, latent_size, latent_size)\ngen_latents_base = diffusion_gen.sample(dit_gen_base, latent_shape, progress=True)\n\nprint(\"Generating MM-Reg samples...\")\ngen_latents_mm = diffusion_gen.sample(dit_gen_mm, latent_shape, progress=True)\n\n# Decode to images\ndef decode_and_plot(vae, latents, title, save_path):\n    vae.eval()\n    with torch.no_grad():\n        images = vae.decode(latents.to(device))\n\n    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n    fig.suptitle(title, fontsize=14)\n    for i in range(16):\n        img = images[i].cpu().permute(1, 2, 0).numpy()\n        img = ((img + 1) / 2).clip(0, 1)\n        axes[i // 8, i % 8].imshow(img)\n        axes[i // 8, i % 8].axis('off')\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n\ndecode_and_plot(vae_baseline, gen_latents_base,\n               \"Generated Faces - Baseline (DiT-S)\",\n               './checkpoints/celeba_generated_baseline.png')\n\ndecode_and_plot(vae_mmreg, gen_latents_mm,\n               \"Generated Faces - MM-Reg (DiT-S)\",\n               './checkpoints/celeba_generated_mmreg.png')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": "## 7. Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "def to_python(val):\n    if hasattr(val, 'item'):\n        return val.item()\n    return float(val)\n\nsummary = {\n    'config': CONFIG,\n    'vae_results': {\n        'baseline': {k: to_python(v) for k, v in vae_results_baseline.items()},\n        'mmreg': {k: to_python(v) for k, v in vae_results_mmreg.items()}\n    },\n    'diffusion_results': {\n        'baseline_train': to_python(trainer_diff_baseline.train_history[-1]['loss']),\n        'baseline_val': to_python(base_val),\n        'mmreg_train': to_python(trainer_diff_mmreg.train_history[-1]['loss']),\n        'mmreg_val': to_python(mm_val),\n        'improvement_pct': to_python(improvement)\n    }\n}\n\nwith open('./checkpoints/celeba_experiment_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"=\"*60)\nprint(\"CELEBA EXPERIMENT SUMMARY\")\nprint(\"=\"*60)\n\nprint(f\"\\nVAE (trained on full {total_train:,} images):\")\nprint(f\"  Baseline: Recon MSE={vae_results_baseline['recon_mse']:.6f}, Pearson={vae_results_baseline['pearson']:.4f}\")\nprint(f\"  MM-Reg:   Recon MSE={vae_results_mmreg['recon_mse']:.6f}, Pearson={vae_results_mmreg['pearson']:.4f}\")\n\nprint(f\"\\nDiffusion ({CONFIG['diffusion_epochs']} epochs):\")\nprint(f\"  Baseline val loss: {base_val:.6f}\")\nprint(f\"  MM-Reg val loss:   {mm_val:.6f}\")\nprint(f\"  MM-Reg improvement: {improvement:.1f}%\")\n\nprint(f\"\\nResults saved to ./checkpoints/celeba_experiment_summary.json\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}