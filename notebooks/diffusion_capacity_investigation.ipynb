{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model Capacity Investigation\n",
    "\n",
    "Does a larger UNet improve diffusion quality? Test 3 model sizes on 20k CelebA latents.\n",
    "\n",
    "**Assumes:** VAEs already trained, latents encoded (from scaling experiment notebook).\n",
    "\n",
    "**Improvements over previous training:**\n",
    "- Gradient clipping (max_norm=1.0)\n",
    "- EMA weights (decay=0.9999)\n",
    "- Cosine LR schedule\n",
    "\n",
    "## UNet Variants\n",
    "| Variant | base_channels | channel_mult | num_res_blocks |\n",
    "|---------|--------------|--------------|----------------|\n",
    "| Small   | 128          | (1, 2, 4)    | 2              |\n",
    "| Medium  | 192          | (1, 2, 4)    | 2              |\n",
    "| Large   | 256          | (1, 2, 4, 4) | 2              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Run this cell if starting fresh. Skip if continuing from scaling experiment notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ONLY run this cell if starting fresh (not continuing from scaling notebook) ===\n",
    "# If you already have latents in memory, skip to cell 2.\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "\n",
    "# Load cached latents from scaling experiment\n",
    "print(\"\\nLoading cached latents...\")\n",
    "train_latents_baseline = torch.load('./embeddings/scaling_train_latents_baseline.pt')\n",
    "val_latents_baseline = torch.load('./embeddings/scaling_val_latents_baseline.pt')\n",
    "train_latents_mmreg = torch.load('./embeddings/scaling_train_latents_mmreg.pt')\n",
    "val_latents_mmreg = torch.load('./embeddings/scaling_val_latents_mmreg.pt')\n",
    "print(f\"Train baseline: {train_latents_baseline.shape}\")\n",
    "print(f\"Train MM-Reg: {train_latents_mmreg.shape}\")\n",
    "\n",
    "# Load VAEs for decoding generated samples\n",
    "from src.models.vae_wrapper import load_vae\n",
    "vae_baseline = load_vae(device=device)\n",
    "vae_baseline.load_state_dict(torch.load('./checkpoints/scaling_baseline_vae/best.pt', map_location=device)['model_state_dict'])\n",
    "vae_baseline.eval()\n",
    "\n",
    "vae_mmreg = load_vae(device=device)\n",
    "vae_mmreg.load_state_dict(torch.load('./checkpoints/scaling_mmreg_vae/best.pt', map_location=device)['model_state_dict'])\n",
    "vae_mmreg.eval()\n",
    "print(\"VAEs loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from src.models.diffusion import SimpleUNet, GaussianDiffusion\n",
    "from src.diffusion_trainer import DiffusionTrainer, LatentDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Experiment config\n",
    "N_TRAIN = 20000\n",
    "DIFFUSION_EPOCHS = 50\n",
    "DIFFUSION_LR = 1e-4\n",
    "DIFFUSION_TIMESTEPS = 1000\n",
    "BATCH_SIZE = 64\n",
    "GRAD_CLIP = 1.0\n",
    "EMA_DECAY = 0.9999\n",
    "\n",
    "# UNet variants\n",
    "UNET_CONFIGS = {\n",
    "    'small': {\n",
    "        'base_channels': 128,\n",
    "        'channel_mult': (1, 2, 4),\n",
    "        'num_res_blocks': 2,\n",
    "    },\n",
    "    'medium': {\n",
    "        'base_channels': 192,\n",
    "        'channel_mult': (1, 2, 4),\n",
    "        'num_res_blocks': 2,\n",
    "    },\n",
    "    'large': {\n",
    "        'base_channels': 256,\n",
    "        'channel_mult': (1, 2, 4, 4),\n",
    "        'num_res_blocks': 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Subsample 20k training latents\n",
    "sub_train_baseline = train_latents_baseline[:N_TRAIN]\n",
    "sub_train_mmreg = train_latents_mmreg[:N_TRAIN]\n",
    "latent_size = sub_train_baseline.shape[2]\n",
    "\n",
    "# Count params for each config\n",
    "print(f\"Training on {N_TRAIN:,} samples, {DIFFUSION_EPOCHS} epochs\")\n",
    "print(f\"Latent size: {latent_size}x{latent_size}\")\n",
    "print(f\"\\nUNet variants:\")\n",
    "for name, cfg in UNET_CONFIGS.items():\n",
    "    tmp = SimpleUNet(in_channels=4, **cfg)\n",
    "    n_params = sum(p.numel() for p in tmp.parameters())\n",
    "    print(f\"  {name:>6}: {n_params/1e6:.1f}M params  (base={cfg['base_channels']}, mult={cfg['channel_mult']})\")\n",
    "    del tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop with EMA + Gradient Clipping + Cosine LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"Update EMA model weights.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for ema_p, p in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_p.data.mul_(decay).add_(p.data, alpha=1 - decay)\n",
    "\n",
    "\n",
    "def train_diffusion(\n",
    "    train_latents, val_latents, unet_config, name,\n",
    "    epochs=DIFFUSION_EPOCHS, lr=DIFFUSION_LR,\n",
    "    batch_size=BATCH_SIZE, grad_clip=GRAD_CLIP, ema_decay=EMA_DECAY\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a diffusion model with improved procedure.\n",
    "    Returns: dict with model, ema_model, train_history, val_history.\n",
    "    \"\"\"\n",
    "    save_dir = Path(f'./checkpoints/capacity_{name}')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create model\n",
    "    diffusion = GaussianDiffusion(num_timesteps=DIFFUSION_TIMESTEPS, device=device)\n",
    "    unet = SimpleUNet(in_channels=4, **unet_config).to(device)\n",
    "    ema = copy.deepcopy(unet)\n",
    "    for p in ema.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    n_params = sum(p.numel() for p in unet.parameters())\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {name} ({n_params/1e6:.1f}M params)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Optimizer + scheduler\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        LatentDataset(train_latents), batch_size=batch_size,\n",
    "        shuffle=True, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        LatentDataset(val_latents), batch_size=batch_size,\n",
    "        shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Train ---\n",
    "        unet.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for latents in train_loader:\n",
    "            latents = latents.to(device)\n",
    "            bs = latents.shape[0]\n",
    "            t = torch.randint(0, DIFFUSION_TIMESTEPS, (bs,), device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = diffusion.p_losses(unet, latents, t)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(unet.parameters(), max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            update_ema(ema, unet, ema_decay)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss = total_loss / num_batches\n",
    "        train_history.append(train_loss)\n",
    "\n",
    "        # --- Validate (using EMA model) ---\n",
    "        ema.eval()\n",
    "        val_total = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for latents in val_loader:\n",
    "                latents = latents.to(device)\n",
    "                bs = latents.shape[0]\n",
    "                t = torch.randint(0, DIFFUSION_TIMESTEPS, (bs,), device=device)\n",
    "                loss = diffusion.p_losses(ema, latents, t)\n",
    "                val_total += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        val_loss = val_total / val_batches\n",
    "        val_history.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({'model_state_dict': ema.state_dict()}, save_dir / 'best.pt')\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            lr_now = scheduler.get_last_lr()[0]\n",
    "            print(f\"  Epoch {epoch:3d}: train={train_loss:.6f}  val={val_loss:.6f}  lr={lr_now:.2e}\")\n",
    "\n",
    "    print(f\"  Best val loss: {best_val_loss:.6f}\")\n",
    "\n",
    "    return {\n",
    "        'diffusion': diffusion,\n",
    "        'ema_model': ema,\n",
    "        'train_history': train_history,\n",
    "        'val_history': val_history,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'n_params': n_params,\n",
    "        'config': unet_config,\n",
    "        'save_dir': save_dir,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run All 6 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for size_name, unet_cfg in UNET_CONFIGS.items():\n",
    "    # Baseline\n",
    "    key_base = f'{size_name}_baseline'\n",
    "    results[key_base] = train_diffusion(\n",
    "        sub_train_baseline, val_latents_baseline,\n",
    "        unet_cfg, key_base\n",
    "    )\n",
    "\n",
    "    # MM-Reg\n",
    "    key_mm = f'{size_name}_mmreg'\n",
    "    results[key_mm] = train_diffusion(\n",
    "        sub_train_mmreg, val_latents_mmreg,\n",
    "        unet_cfg, key_mm\n",
    "    )\n",
    "\n",
    "    # Free training model from GPU (keep EMA for generation)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "print(f\"{'Model':>16} | {'Params':>8} | {'Base Val':>10} | {'MMReg Val':>10} | {'Improv.':>8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for size_name in UNET_CONFIGS:\n",
    "    base_val = results[f'{size_name}_baseline']['best_val_loss']\n",
    "    mm_val = results[f'{size_name}_mmreg']['best_val_loss']\n",
    "    n_params = results[f'{size_name}_baseline']['n_params']\n",
    "    improvement = (base_val - mm_val) / base_val * 100\n",
    "    print(f\"{size_name:>16} | {n_params/1e6:>7.1f}M | {base_val:>10.6f} | {mm_val:>10.6f} | {improvement:>7.1f}%\")\n",
    "\n",
    "# Also show absolute improvement from small to large\n",
    "print(f\"\\nBaseline improvement (small -> large): \"\n",
    "      f\"{results['small_baseline']['best_val_loss']:.6f} -> {results['large_baseline']['best_val_loss']:.6f} \"\n",
    "      f\"({(results['small_baseline']['best_val_loss'] - results['large_baseline']['best_val_loss']) / results['small_baseline']['best_val_loss'] * 100:.1f}%)\")\n",
    "print(f\"MM-Reg improvement (small -> large): \"\n",
    "      f\"{results['small_mmreg']['best_val_loss']:.6f} -> {results['large_mmreg']['best_val_loss']:.6f} \"\n",
    "      f\"({(results['small_mmreg']['best_val_loss'] - results['large_mmreg']['best_val_loss']) / results['small_mmreg']['best_val_loss'] * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training curves - all 6 runs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "for i, size_name in enumerate(UNET_CONFIGS):\n",
    "    ax = axes[i]\n",
    "    n_params = results[f'{size_name}_baseline']['n_params']\n",
    "\n",
    "    ax.plot(results[f'{size_name}_baseline']['val_history'], 'b-', label='Baseline', alpha=0.8, linewidth=1.5)\n",
    "    ax.plot(results[f'{size_name}_mmreg']['val_history'], 'r-', label='MM-Reg', alpha=0.8, linewidth=1.5)\n",
    "\n",
    "    ax.set_title(f'{size_name.capitalize()} ({n_params/1e6:.1f}M)', fontsize=13)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Val Loss (EMA)')\n",
    "\n",
    "plt.suptitle(f'Diffusion Val Loss by Model Size ({N_TRAIN//1000}k samples)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./checkpoints/capacity_val_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Best val loss vs model size\n",
    "sizes = list(UNET_CONFIGS.keys())\n",
    "params = [results[f'{s}_baseline']['n_params'] / 1e6 for s in sizes]\n",
    "base_vals = [results[f'{s}_baseline']['best_val_loss'] for s in sizes]\n",
    "mm_vals = [results[f'{s}_mmreg']['best_val_loss'] for s in sizes]\n",
    "improvements = [(b - m) / b * 100 for b, m in zip(base_vals, mm_vals)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Left: val loss vs params\n",
    "axes[0].plot(params, base_vals, 'bo-', label='Baseline', markersize=10, linewidth=2)\n",
    "axes[0].plot(params, mm_vals, 'rs-', label='MM-Reg', markersize=10, linewidth=2)\n",
    "axes[0].set_xlabel('Model Parameters (M)', fontsize=12)\n",
    "axes[0].set_ylabel('Best Val Loss', fontsize=12)\n",
    "axes[0].set_title('Diffusion Quality vs Model Capacity', fontsize=13)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "for j, s in enumerate(sizes):\n",
    "    axes[0].annotate(s, (params[j], base_vals[j]), textcoords='offset points',\n",
    "                     xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "# Right: MM-Reg improvement % by size\n",
    "x = range(len(sizes))\n",
    "axes[1].bar(x, improvements, color=['#2ecc71', '#27ae60', '#1e8449'], alpha=0.8)\n",
    "axes[1].set_xlabel('Model Size', fontsize=12)\n",
    "axes[1].set_ylabel('MM-Reg Improvement (%)', fontsize=12)\n",
    "axes[1].set_title('MM-Reg Advantage by Model Capacity', fontsize=13)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f'{s}\\n({p:.0f}M)' for s, p in zip(sizes, params)])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for j, v in enumerate(improvements):\n",
    "    axes[1].text(j, v + 0.5, f'{v:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./checkpoints/capacity_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Samples (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model size based on lowest val loss across both variants\n",
    "best_size = min(UNET_CONFIGS.keys(), key=lambda s: results[f'{s}_mmreg']['best_val_loss'])\n",
    "print(f\"Best model size: {best_size}\")\n",
    "\n",
    "def decode_and_plot(vae, latents, title, save_path):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        images = vae.decode(latents.to(device))\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    for i in range(16):\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = ((img + 1) / 2).clip(0, 1)\n",
    "        axes[i // 8, i % 8].imshow(img)\n",
    "        axes[i // 8, i % 8].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Generate with each model size for comparison\n",
    "latent_shape = (16, 4, latent_size, latent_size)\n",
    "\n",
    "for size_name in UNET_CONFIGS:\n",
    "    print(f\"\\n--- Generating with {size_name} model ---\")\n",
    "    \n",
    "    # Load best EMA checkpoint\n",
    "    diff = results[f'{size_name}_mmreg']['diffusion']\n",
    "    \n",
    "    ema_mm = SimpleUNet(in_channels=4, **UNET_CONFIGS[size_name]).to(device)\n",
    "    ckpt = torch.load(f'./checkpoints/capacity_{size_name}_mmreg/best.pt', map_location=device)\n",
    "    ema_mm.load_state_dict(ckpt['model_state_dict'])\n",
    "    \n",
    "    gen_latents = diff.sample(ema_mm, latent_shape, progress=True)\n",
    "    \n",
    "    n_params = results[f'{size_name}_mmreg']['n_params']\n",
    "    decode_and_plot(\n",
    "        vae_mmreg, gen_latents,\n",
    "        f'MM-Reg {size_name.capitalize()} ({n_params/1e6:.0f}M params)',\n",
    "        f'./checkpoints/capacity_samples_{size_name}_mmreg.png'\n",
    "    )\n",
    "    \n",
    "    del ema_mm, gen_latents\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_python(val):\n",
    "    if hasattr(val, 'item'):\n",
    "        return val.item()\n",
    "    return float(val)\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'diffusion_capacity_investigation',\n",
    "    'n_train': N_TRAIN,\n",
    "    'epochs': DIFFUSION_EPOCHS,\n",
    "    'improvements': ['gradient_clipping', 'ema', 'cosine_lr'],\n",
    "    'results': {}\n",
    "}\n",
    "\n",
    "for size_name in UNET_CONFIGS:\n",
    "    base_val = results[f'{size_name}_baseline']['best_val_loss']\n",
    "    mm_val = results[f'{size_name}_mmreg']['best_val_loss']\n",
    "    summary['results'][size_name] = {\n",
    "        'n_params': results[f'{size_name}_baseline']['n_params'],\n",
    "        'config': {k: list(v) if isinstance(v, tuple) else v for k, v in UNET_CONFIGS[size_name].items()},\n",
    "        'baseline_best_val': to_python(base_val),\n",
    "        'mmreg_best_val': to_python(mm_val),\n",
    "        'improvement_pct': to_python((base_val - mm_val) / base_val * 100),\n",
    "    }\n",
    "\n",
    "with open('./checkpoints/capacity_investigation_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CAPACITY INVESTIGATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining: {N_TRAIN:,} samples, {DIFFUSION_EPOCHS} epochs\")\n",
    "print(f\"Improvements: grad clip={GRAD_CLIP}, EMA={EMA_DECAY}, cosine LR\")\n",
    "print(f\"\\n{'Model':>8} | {'Params':>8} | {'Base Val':>10} | {'MMReg Val':>10} | {'Improv':>7} | {'Base Drop':>10}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "small_base = results['small_baseline']['best_val_loss']\n",
    "small_mm = results['small_mmreg']['best_val_loss']\n",
    "\n",
    "for size_name in UNET_CONFIGS:\n",
    "    base_val = results[f'{size_name}_baseline']['best_val_loss']\n",
    "    mm_val = results[f'{size_name}_mmreg']['best_val_loss']\n",
    "    n_params = results[f'{size_name}_baseline']['n_params']\n",
    "    improv = (base_val - mm_val) / base_val * 100\n",
    "    base_drop = (small_base - base_val) / small_base * 100\n",
    "    print(f\"{size_name:>8} | {n_params/1e6:>7.1f}M | {base_val:>10.6f} | {mm_val:>10.6f} | {improv:>6.1f}% | {base_drop:>+9.1f}%\")\n",
    "\n",
    "print(f\"\\nResults saved to ./checkpoints/capacity_investigation_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
